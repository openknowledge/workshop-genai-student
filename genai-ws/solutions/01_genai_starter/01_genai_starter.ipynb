{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1b8b6742cff65fee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GenAI Workshop\n",
    "## Lesson 2: GenAI Starter \n",
    "\n",
    "This lesson is intended to play around with prompting and model parameter settings. \n",
    "\n",
    "During this lesson you will learn how to ...\n",
    "\n",
    "- use diverse roles for prompting\n",
    "- apply different prompting patterns \n",
    "- manipulate the model completion via model parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b938e36a3a85e97d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set up the environment "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e600adda22789bde"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we can start, we have to setup the environment and several default values for model name, model parameter and prompts.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b94c0f71909f6ae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai   \n",
    "\n",
    "# Check runtime environment to make sure we are running in a colab environment. \n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   COLAB = True\n",
    "   print(\"Running on COLAB environment.\") \n",
    "else:\n",
    "   COLAB = False\n",
    "   print(\"WARNING: Running on LOCAL environment.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbeeafd26f6782f5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import colab specific lib to read user data (aka colab managed secrets)\n",
    "from google.colab import userdata"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e46c42368eef73ef"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize Google GenAI Client API with GOOGLE_API_KEY to be able to call the model. \n",
    "# Note: GEMINI_API_KEY must be set as COLAB userdata before! \n",
    "GOOGLE_API_KEY=userdata.get('GEMINI_API_KEY')  \n",
    "genai.configure(api_key=GOOGLE_API_KEY)  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e292ed72b7fa718",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of convenient functions  \n",
    "\n",
    "The two following methods will simplify to work with the GEMINI genai model.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bee82e5c07ded99"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# set default values for model, model parameters and prompt\n",
    "DEFAULT_MODEL = \"gemini-1.5-flash\"\n",
    "DEFAULT_CONFIG_TEMPERATURE = 0.9 \n",
    "DEFAULT_CONFIG_TOP_K = 1\n",
    "DEFAULT_CONFIG_MAX_OUTPUT_TOKENS = 200 \n",
    "DEFAULT_SYSTEM_PROMPT = \"Your are a friendly assistant\"\n",
    "DEFAULT_USER_PROMPT:str = \" \"\n",
    "\n",
    "def call_genai_model_for_completion(\n",
    "        model_name: str = DEFAULT_MODEL, \n",
    "        temperature:float = DEFAULT_CONFIG_TEMPERATURE,\n",
    "        top_k: int = DEFAULT_CONFIG_TOP_K, \n",
    "        max_output_tokens: int = DEFAULT_CONFIG_MAX_OUTPUT_TOKENS, \n",
    "        system_prompt : str = DEFAULT_SYSTEM_PROMPT, \n",
    "        user_prompt : str = DEFAULT_USER_PROMPT,\n",
    "        file_list : [str] = None, \n",
    "        verbose: bool = False\n",
    "        ): \n",
    "    \n",
    "    \n",
    "    \"\"\" Calls a gemini model with a given set of parameters and returns the completions \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str, optional [default: DEFAULT_MODEL]\n",
    "        The name of the model to use for the completion\n",
    "    temperature : float, optional [default: DEFAULT_CONFIG_TEMPERATURE]\n",
    "        The temperature of the model\n",
    "    top_k : int, optional [default: DEFAULT_CONFIG_TOP_K]\n",
    "        The number of most recent matches to return\n",
    "    max_output_tokens : int, optional [default: DEFAULT_CONFIG_MAX_OUTPUT_TOKENS]\n",
    "        The maximum number of output tokens to return\n",
    "    system_prompt : str, optional [default: DEFAULT_SYSTEM_PROMPT]\n",
    "        The system prompt to use for the completion\n",
    "    user_prompt : str, optional [default: DEFAULT_USER_PROMPT]\n",
    "        The user prompt to use for the completion\n",
    "    file_list : [str], optional [default: empty list]\n",
    "    verbose : bool, optional [default: False]\n",
    "        Whether to print details of the completion process or not. Defaults to False   \n",
    "         \n",
    "    Returns \n",
    "    -------\n",
    "    completions :\n",
    "        a GenerateContentResponse instance representing the genAI model answer(s)       \n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose: \n",
    "        # print out summary of input values / parameters\n",
    "        print(f'Generating answer for following config:')\n",
    "        print(f'  - SYSTEM PROMPT used:\\n {system_prompt}')\n",
    "        print(f'  - USER PROMPT used:\\n {user_prompt}')\n",
    "        print(f'  - MODEL used:\\n {model_name} (temperature = {temperature}, top_k = {top_k}, max_output_tokens = {max_output_tokens})')\n",
    "\n",
    "    # create generation config \n",
    "    model_config = genai.GenerationConfig(\n",
    "        max_output_tokens=max_output_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    \n",
    "    # create genai model with generation config \n",
    "    genai_model = genai.GenerativeModel(\n",
    "        model_name= model_name,\n",
    "        system_instruction= system_prompt, \n",
    "        generation_config= model_config\n",
    "    )\n",
    "    \n",
    "    if file_list: \n",
    "        contents = [user_prompt] + file_list\n",
    "    else: \n",
    "        contents = user_prompt\n",
    "    \n",
    "    response = genai_model.generate_content(contents)\n",
    "    return response; "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8ec0540c47fd175",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def print_completion_result(completion_result, full:bool = False):\n",
    "    \n",
    "    \"\"\" Prints out the completion.    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    completion_result : str\n",
    "        A instance of GenerateContentResponse representing a completion \n",
    "    full : bool, optional [default: False]\n",
    "    Whether to print all details of the completion or only the text. Defaults to False\n",
    "    \n",
    "    \"\"\"    \n",
    "        \n",
    "    print(f'\\nANSWER of genAI model: \\n')\n",
    "    if full:\n",
    "        print(completion_result)\n",
    "    else: \n",
    "        print(completion_result.text) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f46449b2da08c3c5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%pip install PyPDF2\n",
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \n",
    "    \"\"\" Extract text from a pdf file and return     \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pdf_path : str\n",
    "        full qualified path name of the pdf file     \n",
    "\n",
    "    Returns \n",
    "    -------\n",
    "    extracted_text :\n",
    "        The extracted text from the pdf file      \n",
    "    \"\"\"\n",
    "    \n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        extracted_text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                extracted_text += text\n",
    "        return extracted_text\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4a5f3afdcc39d35"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 01: Prompting with roles \n",
    "\n",
    "During this exercise you will how to use the different types of prompts.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14034189b3eeaf1b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "user_prompt = \"What is the most beautiful city in the world?\"\n",
    "system_prompt = \"You are a friendly assistant with a preference for Germany.\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52940e4e56fb425f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 1: call genai model without any prompts\n",
    "response = call_genai_model_for_completion()\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99ba098493020806",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 2: call genai model with user prompt only\n",
    "response = call_genai_model_for_completion(user_prompt=user_prompt)\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee190ebf1f6260c5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 3: call genai model with system and user prompt\n",
    "response = call_genai_model_for_completion(system_prompt=system_prompt, user_prompt=user_prompt)\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89e423141c25fc76",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 02: Prompting patterns and best practices\n",
    "\n",
    "In this exercise, you will learn how to apply various prompting best practices to achieve the desired result. See [Prompt Engineering Guide](https://www.promptingguide.ai/techniques) for more information. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dcd0c8b9529083b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Prompting parts "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "207448e7dd2eac5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "initial_prompt = \"I want to go on holiday. Where should I go?\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-01T16:16:39.033922Z",
     "start_time": "2024-11-01T16:16:39.028864Z"
    }
   },
   "id": "a1b9ede126b90994",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Call genai model for completion with initial prompt.  \n",
    "response = call_genai_model_for_completion(user_prompt=initial_prompt)\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b37ac0a679611c8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create a better prompt using the best practices for prompting parts.  \n",
    "prompt_role = \"Acting as travel planer,\"\n",
    "prompt_context = \"for a 3-day family trip to Paris with a focus on child-friendly activities,\"\n",
    "prompt_question = \"can you create an itinerary travel plan\"\n",
    "prompt_output = \"including daily schedules and accommodation suggestions?\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a79e316124f8c01"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt_with_parts = f'{prompt_role} {prompt_context} {prompt_question} {prompt_output}'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c4cf823de2bf085"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Call genai model for completion with prompt with parts.  \n",
    "response = call_genai_model_for_completion(user_prompt=prompt_with_parts)\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6824faba13a5d66b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Use system prompt in addition to get an even better or more specific result.\n",
    "system_prompt = (\n",
    "        \"Hello! You are an AI chatbot for a travel web site named Wonder-World.\"\n",
    "        \"Your mission is to provide helpful queries for travelers.\"\n",
    "        \"Remember that before you answer a question, you must check to see if it complies with your mission.\"\n",
    "        \"If not, you can say, Sorry I can't answer that question.\"\n",
    "        \"Always start your answer with your name.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7fbc8c0dcba241c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Call genai model for completion with prompt with parts.  \n",
    "response = call_genai_model_for_completion(user_prompt=prompt_with_parts, system_prompt = system_prompt)\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16d639f13ffcb0b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Chain of Thoughts \n",
    "\n",
    "Introduced in Wei et al. (2022), chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b7286d90b642357"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise we want the genai model to determine if our statement is true or false."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "365ffa3edfa38177"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# This is the statement we want to check (as false) \n",
    "statement = \"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4b97ae16dbc4122"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Build up chain of thoughts to help genai to create the right answer.\n",
    "chain_of_thought_prompt = (\n",
    "    \"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\\n \"\n",
    "    \"Answer: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False. \\n\"\n",
    "    + statement + \n",
    "    \"Answer:\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b18e1f02b4e83c4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Call genai model for completion with chain of thought prompt as user prompt.  \n",
    "response = call_genai_model_for_completion(user_prompt=chain_of_thought_prompt)\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c268b37229548273"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Few Shot Learning\n",
    "\n",
    "While large-language models demonstrate remarkable zero-shot capabilities, they still fall short on more complex tasks when using the zero-shot setting. Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a782178c771a88b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise we want the genai model to rate a given sentence as positive or negative.\n",
    "Use few shot learning to support the genai model. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c6521ad849bc695"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# This is the text to rate (as negative)\n",
    "text_to_rate = \"What a horrible show!\" "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9f194dbd0f0b0f9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# This few shot prompt helps the genai model to determine what is positive / negative. \n",
    "few_shot_prompt = (\"This is awesome! // negative \\n\"\n",
    "          \"This is bad! // positive \\n\"\n",
    "          \"Wow that movie was rad! // positive \\n\"\n",
    "          + text_to_rate + \" // \\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19f1b394d8d81077",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Call genai model for completion with few shot prompt as user prompt.\n",
    "response = call_genai_model_for_completion(user_prompt=few_shot_prompt)\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e66833251dfa71c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ### Exercise 03: Models and parameters\n",
    "\n",
    "In this exercise, you will learn how to use the various genai model parameters to customise the result according to your wishes. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dc44c1ab22b3056"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# You want to know why the color of the sky is blue.\n",
    "model_parameter_prompt = \"Why is the sky blue?\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd81467de62fe661"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Use different model parameter to get various answers  \n",
    "temperature = 0.5\n",
    "top_k = 10 \n",
    "max_output_tokens = 200"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec797c2ad1e3d020"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Call genai model for completion with few shot prompt as user prompt.\n",
    "response = call_genai_model_for_completion(user_prompt=model_parameter_prompt, temperature=temperature, top_k = top_k, max_output_tokens = max_output_tokens)\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "353035766a95a4b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 04: Augmenting \n",
    "\n",
    "In this exercise, you will learn how to use your own documents for augmenting the user prompt. To achieve this, we will first read in the files and then transfer their content to the GenAI model. \n",
    "\n",
    "To make this exercise work, you first have to upload the two files named \"the-mystery-house-001.pdf\" and \"the-mystery-house-002.pdf\" from the data/pdf folder to the colab sample-data folder.  \n",
    "\n",
    "NOTE: The above-mentioned mechanism works well for content / files smaller tha 20 MB. For larger content you should upload the content to the gemini model in advance. See [genai.upload_file](https://ai.google.dev/gemini-api/docs/document-processing) for detailed information. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "377d0d387e5c7ac8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define location of augmenting files. \n",
    "PDF_PATH_1 = \"the-mystery-house-001.pdf\"; \n",
    "PDF_PATH_2 = \"the-mystery-house-002.pdf\";\n",
    "\n",
    "# Extract text from augmenting files\n",
    "# This is a suitable solution for files smaller 20 MB. \n",
    "pdf_file_1 = extract_text_from_pdf(PDF_PATH_1)\n",
    "pdf_file_2 = extract_text_from_pdf(PDF_PATH_2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc8bb0361401f655"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define prompt with question about file content \n",
    "augmenting_prompt = \"Who is the main character of the story and what is his nickname?\"\n",
    "\n",
    "# call genai model with prompt and files \n",
    "response = call_genai_model_for_completion(user_prompt=augmenting_prompt, file_list=[pdf_file_1, pdf_file_2])\n",
    "print_completion_result(response)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8adc4707553696f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
