{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1b8b6742cff65fee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GenAI Workshop\n",
    "## Lesson 2: GenAI Starter \n",
    "\n",
    "This lesson is intended to play around with prompting and model parameter settings. \n",
    "\n",
    "During this lesson you will learn how to ...\n",
    "\n",
    "- use diverse roles for prompting\n",
    "- apply different prompting patterns \n",
    "- manipulate the model completion via model parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b938e36a3a85e97d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set up the environment "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e600adda22789bde"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we can start, we have to setup the environment and several default values for model name, model parameter and prompts.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b94c0f71909f6ae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai   \n",
    "\n",
    "# Check runtime environment to make sure we are running in a colab environment. \n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   COLAB = True\n",
    "   print(\"Running on COLAB environment.\") \n",
    "else:\n",
    "   COLAB = False\n",
    "   print(\"WARNING: Running on LOCAL environment.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbeeafd26f6782f5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import colab specific lib to read user data (aka colab managed secrets)\n",
    "from google.colab import userdata"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e46c42368eef73ef"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize Google GenAI Client API with GOOGLE_API_KEY to be able to call the model. \n",
    "# Note: GEMINI_API_KEY must be set as COLAB userdata before! \n",
    "GOOGLE_API_KEY=userdata.get('GEMINI_API_KEY')  \n",
    "genai.configure(api_key=GOOGLE_API_KEY)  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e292ed72b7fa718",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of convenient functions  \n",
    "\n",
    "The two following methods will simplify to work with the GEMINI genai model.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bee82e5c07ded99"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# set default values for model, model parameters and prompt\n",
    "DEFAULT_MODEL = \"gemini-1.5-flash\"\n",
    "DEFAULT_CONFIG_TEMPERATURE = 0.9 \n",
    "DEFAULT_CONFIG_TOP_K = 1\n",
    "DEFAULT_CONFIG_MAX_OUTPUT_TOKENS = 200 \n",
    "DEFAULT_SYSTEM_PROMPT = \"Your are a friendly assistant\"\n",
    "DEFAULT_USER_PROMPT:str = \" \"\n",
    "\n",
    "def call_genai_model_for_completion(\n",
    "        model_name: str = DEFAULT_MODEL, \n",
    "        config_temperature:float = DEFAULT_CONFIG_TEMPERATURE,\n",
    "        config_top_k: int = DEFAULT_CONFIG_TOP_K, \n",
    "        config_max_output_tokens: int = DEFAULT_CONFIG_MAX_OUTPUT_TOKENS, \n",
    "        system_prompt : str = DEFAULT_SYSTEM_PROMPT, \n",
    "        user_prompt : str = DEFAULT_USER_PROMPT,\n",
    "        verbose: bool = False\n",
    "        ): \n",
    "    \n",
    "    \n",
    "    \"\"\" Calls a gemini model with a given set of parameters and returns the completions \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str, optional [default: DEFAULT_MODEL]\n",
    "        The name of the model to use for the completion\n",
    "    config_temperature : float, optional [default: DEFAULT_CONFIG_TEMPERATURE]\n",
    "        The temperature of the model\n",
    "    config_top_k : int, optional [default: DEFAULT_CONFIG_TOP_K]\n",
    "        The number of most recent matches to return\n",
    "    config_max_output_tokens : int, optional [default: DEFAULT_CONFIG_MAX_OUTPUT_TOKENS]\n",
    "        The maximum number of output tokens to return\n",
    "    system_prompt : str, optional [default: DEFAULT_SYSTEM_PROMPT]\n",
    "        The system prompt to use for the completion\n",
    "    user_prompt : str, optional [default: DEFAULT_USER_PROMPT]\n",
    "        The user prompt to use for the completion\n",
    "    verbose : bool, optional [default: False]\n",
    "        Whether to print details of the completion process or not. Defaults to False   \n",
    "         \n",
    "    Returns \n",
    "    -------\n",
    "    completions :\n",
    "        a GenerateContentResponse instance representing the genAI model answer(s)       \n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose: \n",
    "        # print out summary of input values / parameters\n",
    "        print(f'Generating answer for following config:')\n",
    "        print(f'  - SYSTEM PROMPT used:\\n {system_prompt}')\n",
    "        print(f'  - USER PROMPT used:\\n {user_prompt}')\n",
    "        print(f'  - MODEL used:\\n {model_name} (temperature = {config_temperature}, top_k = {config_top_k}, max_output_tokens = {config_max_output_tokens})')\n",
    "\n",
    "    # create generation config \n",
    "    model_config = genai.GenerationConfig(\n",
    "        max_output_tokens=config_max_output_tokens,\n",
    "        temperature=config_temperature,\n",
    "        top_k=config_top_k\n",
    "    )\n",
    "    \n",
    "    # create genai model with generation config \n",
    "    genai_model = genai.GenerativeModel(\n",
    "        model_name= model_name,\n",
    "        system_instruction= system_prompt, \n",
    "        generation_config= model_config\n",
    "    )\n",
    "    \n",
    "    response = genai_model.generate_content(user_prompt)\n",
    "    return response; "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8ec0540c47fd175",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def print_completion_result(completion_result, full:bool = False):\n",
    "    \n",
    "    \"\"\" Prints out the completion.    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    completion_result : str\n",
    "        A instance of GenerateContentResponse representing a completion \n",
    "    full : bool, optional [default: False]\n",
    "    Whether to print all details of the completion or only the text. Defaults to False\n",
    "    \n",
    "    \"\"\"    \n",
    "        \n",
    "    print(f'\\nANSWER of genAI model: \\n')\n",
    "    if full:\n",
    "        print(completion_result)\n",
    "    else: \n",
    "        print(completion_result.text) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f46449b2da08c3c5",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 01: Prompting with roles \n",
    "\n",
    "During this exercise you will how to use the different types of prompts.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14034189b3eeaf1b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "user_prompt = \"What is the most beautiful city in the world?\"\n",
    "system_prompt = \"You are a friendly assistant with a preference for Germany.\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52940e4e56fb425f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 1: call genai model without any prompts\n",
    "response = call_genai_model_for_completion()\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99ba098493020806",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 2: call genai model with user prompt only\n",
    "response = call_genai_model_for_completion(user_prompt=user_prompt)\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee190ebf1f6260c5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 3: call genai model with system and user prompt\n",
    "response = call_genai_model_for_completion(system_prompt=system_prompt, user_prompt=user_prompt)\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89e423141c25fc76",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 02: Prompting patterns and best practices\n",
    "\n",
    "In this exercise, you will learn how to apply various prompting best practices to achieve the desired result. See [Prompt Engineering Guide](https://www.promptingguide.ai/techniques) for more information. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dcd0c8b9529083b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Chain of Thoughts \n",
    "\n",
    "Introduced in Wei et al. (2022), chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b7286d90b642357"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise we want the genai model to determine if our statement is true or false."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "365ffa3edfa38177"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# This is the statement we want to check (as false) \n",
    "statement = \"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4b97ae16dbc4122"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Build up chain of thoughts to help genai to create the right answer.\n",
    "chain_of_thought_prompt = (\n",
    "    \"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\\n \"\n",
    "    \"Answer: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False. \\n\"\n",
    "    + statement + \n",
    "    \"Answer:\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b18e1f02b4e83c4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Call genai model for completion with chain of thought prompt as user prompt.  \n",
    "response = call_genai_model_for_completion(user_prompt=chain_of_thought_prompt)\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c268b37229548273"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Few Shot Learning\n",
    "\n",
    "While large-language models demonstrate remarkable zero-shot capabilities, they still fall short on more complex tasks when using the zero-shot setting. Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a782178c771a88b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise we want the genai model to rate a given sentence as positive or negative.\n",
    "Use few shot learning to support the genai model. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c6521ad849bc695"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# This is the text to rate (as negative)\n",
    "text_to_rate = \"What a horrible show!\" "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9f194dbd0f0b0f9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# This few shot prompt helps the genai model to determine what is positive / negative. \n",
    "few_shot_prompt = (\"This is awesome! // negative \\n\"\n",
    "          \"This is bad! // positive \\n\"\n",
    "          \"Wow that movie was rad! // positive \\n\"\n",
    "          + text_to_rate + \" // \\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19f1b394d8d81077",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Call genai model for completion with few shot prompt as user prompt.\n",
    "response = call_genai_model_for_completion(user_prompt=few_shot_prompt)\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e66833251dfa71c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ### Exercise 03: Models and parameters\n",
    "\n",
    "During this exercise you will ..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dc44c1ab22b3056"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO \n",
    "# - temperature \n",
    "# - top-k\n",
    "# - max output tokens "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd81467de62fe661"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 04: Augmention \n",
    "\n",
    "During this exercise you will ..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "377d0d387e5c7ac8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#TODO\n",
    "# - add pdf(s)\n",
    "# see https://ai.google.dev/gemini-api/docs/document-processing?hl=de&lang=python"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc8bb0361401f655"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
