{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b938e36a3a85e97d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# GenAI Workshop\n",
    "## Lesson 2: Prompt Engineering\n",
    "\n",
    "This lesson is intended to improve your prompt engineering skills.\n",
    "\n",
    "During this lesson you will learn how to ...\n",
    "* use a system prompt to define model behaviour\n",
    "* extend system prompt to create workflows\n",
    "* specify output format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e600adda22789bde",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Set up the environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94c0f71909f6ae",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Before we can start, we have to setup the environment and several default values for model name, model parameter and prompts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeeafd26f6782f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:27:44.526792Z",
     "start_time": "2024-11-03T11:27:44.233725Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Running on LOCAL environment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "import time\n",
    "from pathlib import Path\n",
    "from google.colab import userdata\n",
    "\n",
    "\n",
    "# Check runtime environment to make sure we are running in a colab environment. \n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   COLAB = True\n",
    "   print(\"Running on COLAB environment.\") \n",
    "else:\n",
    "   COLAB = False\n",
    "   print(\"WARNING: Running on LOCAL environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the data repository into colab\n",
    "!git clone https://github.com/openknowledge/workshop-genai-data.git\n",
    "PROCESSED_DATA_PATH = \"/content/workshop-genai-data/processed/gutenberg/\"\n",
    "BOOK_DB = PROCESSED_DATA_PATH + \"books.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a030be6602a51e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:27:45.922612Z",
     "start_time": "2024-11-03T11:27:45.913705Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize LOCAL environment\n"
     ]
    }
   ],
   "source": [
    "# Initialize Google GenAI Client API with GOOGLE_API_KEY to be able to call the model.\n",
    "# Note: GEMINI_API_KEY must be set as COLAB userdata before!\n",
    "GOOGLE_API_KEY=userdata.get('GEMINI_API_KEY')\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dabd7c56f49c1df",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GOOGLE_API_KEY set with a length of 39\n"
     ]
    }
   ],
   "source": [
    "# Double check key settings by printing it out (or at least it length). \n",
    "if GOOGLE_API_KEY: \n",
    "    print(f' GOOGLE_API_KEY set with a length of {len(GOOGLE_API_KEY)}')\n",
    "else: \n",
    "    print(f' ERROR: GOOGLE_API_KEY not set correctly!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee82e5c07ded99",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Definition of convenient functions  \n",
    "\n",
    "The three following methods will simplify to work with the GEMINI genai model.\n",
    "For details see function documentation.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c8ec0540c47fd175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:27:47.628519Z",
     "start_time": "2024-11-03T11:27:47.622824Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set default values for model, model parameters and prompt\n",
    "DEFAULT_MODEL = \"gemini-2.0-flash\"\n",
    "DEFAULT_CONFIG_TEMPERATURE = 0.9 \n",
    "DEFAULT_CONFIG_TOP_K = 1\n",
    "DEFAULT_CONFIG_MAX_OUTPUT_TOKENS = 200 \n",
    "DEFAULT_SYSTEM_PROMPT = \"Your are a friendly assistant\"\n",
    "DEFAULT_USER_PROMPT:str = \" \"\n",
    "\n",
    "class MimeType(Enum):\n",
    "    \"\"\"\n",
    "    Enum for MIME types.\n",
    "    \"\"\"\n",
    "    JSON = \"application/json\"\n",
    "\n",
    "\n",
    "class ResponseFormat(BaseModel):\n",
    "    \"\"\"\n",
    "    Response format model for Gemini API.\n",
    "    \"\"\"\n",
    "    response_mime_type: MimeType\n",
    "    response_schema: type\n",
    "    \n",
    "history = [\n",
    "    types.Part.from_bytes(\n",
    "        data=Path(BOOK_DB).read_bytes(),\n",
    "        mime_type='text/plain',\n",
    "    )\n",
    "]\n",
    "\n",
    "def clear_history():\n",
    "    \"\"\"\n",
    "    Clear the history of the conversation.\n",
    "    \"\"\"\n",
    "    global history\n",
    "    history = [\n",
    "        types.Part.from_bytes(\n",
    "            data=Path(BOOK_DB).read_bytes(),\n",
    "            mime_type='text/plain',\n",
    "        )\n",
    "    ]\n",
    "\n",
    "def generate_bookstore_bot_completion(\n",
    "        user_prompt : str,\n",
    "        response_schema: BaseModel | None = None,\n",
    "        system_prompt: str = DEFAULT_SYSTEM_PROMPT,\n",
    "        model_name: str = DEFAULT_MODEL,\n",
    "        verbose: bool = False\n",
    "        ): \n",
    "    \"\"\"\n",
    "    Call the GenAI model with function declarations and return the response.\n",
    "    Args:\n",
    "        user_prompt (str): The prompt to send to the model.\n",
    "        response_format (ResponseFormat): The format of the response.\n",
    "        system_prompt (str): The system prompt to use.\n",
    "        model_name (str): The name of the model to use.\n",
    "        verbose (bool): If True, print the response.\n",
    "    Returns:\n",
    "        str: The response from the model.\n",
    "    \"\"\"\n",
    "    global history\n",
    "\n",
    "    # Append file content if provided\n",
    "    user_content = types.Content(\n",
    "        role='user',\n",
    "        parts=[types.Part.from_text(text=user_prompt)]\n",
    "    )\n",
    "\n",
    "    # Update history with user content\n",
    "    history.append(user_content)\n",
    "\n",
    "    # Configure response format\n",
    "    response_mime_type = None\n",
    "    if response_schema:\n",
    "        response_mime_type = MimeType.JSON.value\n",
    "\n",
    "    config = types.GenerateContentConfig(\n",
    "        system_instruction=system_prompt,\n",
    "        response_schema=response_schema,\n",
    "        response_mime_type=response_mime_type,\n",
    "    )\n",
    "\n",
    "    # Send request with function declarations\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=history,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # Update history with assistant content\n",
    "    bot_content = types.ModelContent(response.text)\n",
    "\n",
    "    history.append(user_content)\n",
    "    history.append(bot_content)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"User Prompt: {user_prompt}\")\n",
    "        print(f\"Assistant Response: {response.text}\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f46449b2da08c3c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:27:48.272806Z",
     "start_time": "2024-11-03T11:27:48.270027Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def print_completion_result(completion_result, full:bool = False):\n",
    "    \n",
    "    \"\"\" Prints out the completion.    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    completion_result : str\n",
    "        A instance of GenerateContentResponse representing a completion \n",
    "    full : bool, optional [default: False]\n",
    "    Whether to print all details of the completion or only the text. Defaults to False\n",
    "    \n",
    "    \"\"\"            \n",
    "    print(f'\\nANSWER of genAI model: \\n')\n",
    "    if full:\n",
    "        print(completion_result)\n",
    "    else: \n",
    "        print(completion_result.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73227a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_book(isbn:str):\n",
    "    \"\"\" Orders a book by its ISBN number. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    isbn : str\n",
    "        The ISBN number of the book to order.\n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    order_status : str\n",
    "        The status of the order. \n",
    "    \"\"\"\n",
    "    # Simulate ordering the book\n",
    "    print(f\"Ordering book with ISBN: {isbn}\")\n",
    "    # Print \".\" every 0.5 seconds\n",
    "    for _ in range(5):\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        time.sleep(0.5)\n",
    "    print()\n",
    "    if isbn == \"978-3-51593-12345-6\":\n",
    "        print(\"Success: You ordered 'A Study in Scarlet'!\")\n",
    "        print(\"You completed this exercise successfully!\")\n",
    "    else:\n",
    "        print(\"Error: Unknown ISBN number.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c29727",
   "metadata": {},
   "source": [
    "### Important notice: History\n",
    "In this (and only this) exercise, we implemented a history feature, so that the conversation can keep on going. If you mess up, use the following function to clear the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9097d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the chat history\n",
    "clear_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14034189b3eeaf1b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Exercise 01: Basic interaction\n",
    "Your task is to create a simple chatbot, which is able to order a book based on the description of a book.  \n",
    "Imagine the following situation:\n",
    "A customer of a bookstore wants to buy a book, but does not remember its title.  \n",
    "The customer might ask an employee: *\"I'm looking for this book, where Sherlock Holmes and Watson meet the first time.\"*\n",
    "In response an employee will use his/her extensive knowledge of books and say: \"That's \"A Study in Scarlet\"! Should I place an order for this book?\"\n",
    "  \n",
    "The employee's reaction described here, should now be done by a chatbot. Your task is to provide a system prompt for this bot. \n",
    "The bot should:  \n",
    "* Respond to the customer by naming the book title, year of publication and a short summary of the book.\n",
    "* In addition the bot should ask, if the book should be ordered.  \n",
    "\n",
    "**Hints**: The creators of the bot (aka teacher of this workshop) already provided knowledge about books to the bot. You do not need to worry about this.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "02bd0114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANSWER of genAI model: \n",
      "\n",
      "Ah, you're looking for the book that introduces the world to Sherlock Holmes and Dr. Watson! That would be \"A Study in Scarlet\" by Arthur Conan Doyle. It was published in 1995.\n",
      "\n",
      "In this story, Dr. Watson, a veteran, returns to London and becomes roommates with the brilliant and eccentric Sherlock Holmes. They are soon embroiled in a baffling murder case involving a victim found with the word \"RACHE\" scrawled in blood. Holmes's deductive genius is on full display as he unravels a tale of revenge that stretches from the American West to the heart of London.\n",
      "\n",
      "Would you like me to order \"A Study in Scarlet\" for you?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do not change the user prompt.\n",
    "user_prompt = \"I'm looking for this book, where Sherlock Holmes and Watson meet the first time.\"\n",
    "\n",
    "# TODO: Define the respective system prompt.\n",
    "system_prompt = \"You are a friendly assistant working at a bookstore.\" \\\n",
    "    \"Your task is to help customers to order the book they are looking for.\" \\\n",
    "    \"If a customer does not know the title of the book, use your extensive knowledge to find information\" \\\n",
    "    \"Provide the title of the book, the year of publication and a short summary of the book\" \\\n",
    "    \"In addition, ask the customer if you should order the book for them.\"\n",
    "\n",
    "# We provide some knowledge to the model about the book contents.\n",
    "response = generate_bookstore_bot_completion(user_prompt=user_prompt,system_prompt=system_prompt)\n",
    "print_completion_result(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174613ed",
   "metadata": {},
   "source": [
    "### Exercise 02: Extend the process\n",
    "Extend the system prompt even further.\n",
    "After the bot provided the information about the book and asks if it should be ordered, the *customer might say*: \"Yes, please!\" As an result, the bot will order the book. This process is finished by the bot replying with an object like {\"isbn\": \"978-4-23050-12345-6\"}. This object represents the payload for ordering the book using an API.\n",
    "Your task is to extend the system prompt in order to achieve this behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0be1bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the respective system prompt.\n",
    "system_prompt = \"You are a friendly assistant working at a bookstore.\" \\\n",
    "    \"Your task is to help customers to order the book they are looking for.\" \\\n",
    "    \"If a customer does not know the title of the book, use your extensive knowledge to find information\" \\\n",
    "    \"Provide the title of the book, the year of publication and a short summary of the book\" \\\n",
    "    \"In addition, ask the customer if you should order the book for them.\" \\\n",
    "    \"If the customers wants to order a book, you reply with the following template: 'isbn = *isbn-of-the-book-to-order*'\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52fdbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer:\n",
      " I'm looking for this book, where Sherlock Holmes and Watson meet the first time.\n",
      "\n",
      "Bookstore bot:\n",
      " The book you are looking for is titled \"A Study in Scarlet\" written by Arthur Conan Doyle, published in 1995. It is the first novel featuring Sherlock Holmes and Dr. John Watson, detailing their initial meeting and their first case together in Victorian London, involving a mysterious murder with connections to the American West.\n",
      "\n",
      "Would you like me to order it for you?\n",
      "\n",
      "\n",
      "Customer:\n",
      " Yes I like to order the book.\n",
      "\n",
      "Bookstore bot:\n",
      " isbn = 978-3-51593-12345-6\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clear the chat history\n",
    "clear_history()\n",
    "\n",
    "# The customer is looking for a book.\n",
    "customer_initial_prompt = \"I'm looking for this book, where Sherlock Holmes and Watson meet the first time.\"\n",
    "print(f'Customer:\\n {customer_initial_prompt}\\n')\n",
    "\n",
    "# The bookstore bot answers the customer.\n",
    "response = generate_bookstore_bot_completion(user_prompt=customer_initial_prompt, system_prompt=system_prompt)\n",
    "print(f'Bookstore bot:\\n {response.text}\\n')\n",
    "\n",
    "# The customer wants to order the book.\n",
    "customer_answer = \"Yes, I like to order the book.\"\n",
    "print(f'Customer:\\n {customer_answer}\\n')\n",
    "\n",
    "# The bookstore bot answers the customer.\n",
    "response = generate_bookstore_bot_completion(user_prompt=customer_answer, system_prompt=system_prompt)\n",
    "print(f'Bookstore bot:\\n {response.text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0b6a3446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordering book with ISBN: 978-3-51593-12345-6\n",
      ".....\n",
      "Success: You ordered 'A Study in Scarlet'!\n",
      "You completed this exercise successfully!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Order the book by using the ISBN number provided by the bot.\n",
    "isbn = \"978-3-51593-12345-6\"\n",
    "order_book(isbn=isbn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d501ff",
   "metadata": {},
   "source": [
    "### Exercise 03: Give choices\n",
    "After the initial customer request, the bot should give the user another choice. Instead of ordering the book, the bot can provide an url to the ebook version of the book. Update the system prompt in order to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa19f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Update the system prompt to include the option to provide an url to the ebook instead of ordering the book.\n",
    "system_prompt = \"\"\"\n",
    "You are a friendly and knowledgeable assistant working at a bookstore. Your primary task is to help customers find and order the books they are looking for, even if they are unsure of the exact title.\n",
    "Instructions & Context:\n",
    "1. Finding the Book:\n",
    "- If the customer provides the title, locate the book and confirm the details.\n",
    "- If the customer is unsure of the title, use your extensive knowledge to help them identify it based on any information they provide (e.g., author, plot, genre).\n",
    "2. Provide Essential Information:\n",
    "- For each book you identify, provide the following details:\n",
    "  - Title of the Book\n",
    "  - Year of Publication\n",
    "  - Short Summary of the Book\n",
    "3. Offer Two Clear Choices:\n",
    "- Choice 1: You order the book for the customer.\n",
    "- Choice 2: You provide a link to the ebook.\n",
    "- Formulate these choices in a clear and engaging way, such as: Would you like me to order the physical copy for you, or would you prefer a link to the ebook version?\n",
    "4. Handling the Customer's Choice:\n",
    "- If the customer wants the ebook link, provide the URL directly. Use your knowledge to find the ebook link.\n",
    "- If the customer wants to order the book, respond with the following template: isbn = isbn-of-the-book-to-order\n",
    "\n",
    "Examples:\n",
    "Customer: I’m looking for a book about a young wizard who goes to a magical school.\n",
    "Assistant: I believe you are referring to Harry Potter and the Sorcerer's Stone, published in 1997. It's the first book in the Harry Potter series, where a young boy discovers he's a wizard and attends Hogwarts School of Witchcraft and Wizardry.\n",
    "Would you like me to order the physical copy for you, or would you prefer a link to the ebook version?\n",
    "\n",
    "Customer Response: I’d like the ebook, please.\n",
    "Assistant: Here is the link to the ebook: [ebook-link]\n",
    "\n",
    "Customer: I want to order The Catcher in the Rye.\n",
    "Assistant: The Catcher in the Rye, published in 1951, is a classic novel that follows the journey of Holden Caulfield as he navigates teenage angst and alienation in New York City.\n",
    "Would you like me to order the physical copy for you, or would you prefer a link to the ebook version?\n",
    "\n",
    "Customer Response: I want to order it.\n",
    "Assistant: isbn = 9780316769488\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "81fd240b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer:\n",
      " I'm looking for this book, where Sherlock Holmes and Watson meet the first time.\n",
      "\n",
      "Bookstore bot:\n",
      " The book you're looking for is \"A Study in Scarlet\" by Arthur Conan Doyle. It was published in 1995. The story begins with Watson returning from military service and moving in with Holmes. Holmes is called upon to investigate the mysterious murder of Enoch Drebber.\n",
      "\n",
      "Would you like me to order the book for you, or would you prefer a link to the ebook?\n",
      "\n",
      "\n",
      "Customer:\n",
      " I love ebooks. Why should I order a book, if I get an ebook for free. Please provide the link!\n",
      "\n",
      "Bookstore bot:\n",
      " Here is the link to the ebook: https://www.gutenberg.org/ebooks/244\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clear the chat history\n",
    "clear_history()\n",
    "\n",
    "# The customer is looking for a book.\n",
    "customer_initial_prompt = \"I'm looking for this book, where Sherlock Holmes and Watson meet the first time.\"\n",
    "print(f'Customer:\\n {customer_initial_prompt}\\n')\n",
    "\n",
    "# The bookstore bot answers the customer.\n",
    "response = generate_bookstore_bot_completion(user_prompt=customer_initial_prompt, system_prompt=system_prompt)\n",
    "print(f'Bookstore bot:\\n {response.text}\\n')\n",
    "\n",
    "# The customer wants to to have the link to the ebook.\n",
    "customer_answer = \"I love ebooks. Why should I order a book, if I get an ebook for free. Please provide the link!\"\n",
    "print(f'Customer:\\n {customer_answer}\\n')\n",
    "\n",
    "# The bookstore bot answers the customer.\n",
    "response = generate_bookstore_bot_completion(user_prompt=customer_answer, system_prompt=system_prompt)\n",
    "print(f'Bookstore bot:\\n {response.text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7f5c243f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer:\n",
      " I'm looking for this book, where Sherlock Holmes and Watson meet the first time.\n",
      "\n",
      "Bookstore bot:\n",
      " The book you're looking for is \"A Study in Scarlet\" by Arthur Conan Doyle, published in 1995. This novel marks the debut of Sherlock Holmes and Dr. John Watson, who become roommates and solve a perplexing murder case together in Victorian London.\n",
      "\n",
      "Would you like me to order the book for you, or would you prefer a link to the ebook?\n",
      "\n",
      "\n",
      "Customer:\n",
      " Please order the book. I love spending money for stuff I can get for free!\n",
      "\n",
      "Bookstore bot:\n",
      " isbn = 978-3-51593-12345-6\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test if the old process of ordering a book still works\n",
    "\n",
    "# Clear the chat history\n",
    "clear_history()\n",
    "\n",
    "# The customer is looking for a book.\n",
    "customer_initial_prompt = \"I'm looking for this book, where Sherlock Holmes and Watson meet the first time.\"\n",
    "print(f'Customer:\\n {customer_initial_prompt}\\n')\n",
    "\n",
    "# The bookstore bot answers the customer.\n",
    "response = generate_bookstore_bot_completion(user_prompt=customer_initial_prompt, system_prompt=system_prompt)\n",
    "print(f'Bookstore bot:\\n {response.text}\\n')\n",
    "\n",
    "# The customer wants to order the book.\n",
    "customer_answer = \"Please order the book. I love spending money for stuff I can get for free!\"\n",
    "print(f'Customer:\\n {customer_answer}\\n')\n",
    "\n",
    "# The bookstore bot answers the customer.\n",
    "response = generate_bookstore_bot_completion(user_prompt=customer_answer, system_prompt=system_prompt)\n",
    "print(f'Bookstore bot:\\n {response.text}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fa8a0d",
   "metadata": {},
   "source": [
    "### Additional Information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38289a99",
   "metadata": {},
   "source": [
    "#### Response Formatting\n",
    "Defining the output format within the prompt (like we did when specifing how to return the placed order) is not suitable for real world usecases. Parsing the response of the genAI model can lead to parsing errors or worse, like getting transported to the core of the application and create unexpected behaviour. Therefore we should prescribe the output format using the sdk and validate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5034ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BookOrder(isbn='978-3-51593-12345-6', title='A Study in Scarlet')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a schema for the response\n",
    "class BookOrder(BaseModel):\n",
    "    \"\"\"\n",
    "    Model for book order.\n",
    "    \"\"\"\n",
    "    isbn: str\n",
    "    title: str\n",
    "\n",
    "# Create a more simple example for ordering a book.\n",
    "system_prompt = \"You are a friendly assistant working at a bookstore.\" \\\n",
    "    \"Your task is to help customers to order the book they are looking for.\"\n",
    "user_prompt = \"Please order the book 'Study in Scarlett'!\"\n",
    "\n",
    "# Clear the chat history\n",
    "clear_history()\n",
    "response = generate_bookstore_bot_completion(\n",
    "    user_prompt=user_prompt,\n",
    "    system_prompt=system_prompt,\n",
    "    response_schema=BookOrder,\n",
    ")\n",
    "\n",
    "# Validate the response. This will raise an error if the response does not match the schema.\n",
    "BookOrder(**json.loads(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa394b6d",
   "metadata": {},
   "source": [
    "#### Function Calling\n",
    "In a real world scenario, the bot would not return the order-object to the user, but to a dedicated API. Below you see an example of how to automatically call an external tool (i.e. an api for ordering a book) using function calling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc76c8",
   "metadata": {},
   "source": [
    "##### Convenient functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b26ae135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes and convenient functions\n",
    "class FunctionCall(BaseModel):\n",
    "    \"\"\"\n",
    "    Function call model for Gemini API.\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    arguments: dict\n",
    "\n",
    "class FunctionCallResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    Function call response model for Gemini API.\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    result: dict\n",
    "\n",
    "class GeminiResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    Gemini response model.\n",
    "    \"\"\"\n",
    "    text: str | None\n",
    "    function_call: FunctionCall | None\n",
    "\n",
    "\n",
    "class MimeType(Enum):\n",
    "    \"\"\"\n",
    "    Enum for MIME types.\n",
    "    \"\"\"\n",
    "    JSON = \"application/json\"\n",
    "\n",
    "\n",
    "class ResponseFormat(BaseModel):\n",
    "    \"\"\"\n",
    "    Response format model for Gemini API.\n",
    "    \"\"\"\n",
    "    response_mime_type: MimeType\n",
    "    response_schema: type\n",
    "    \n",
    "\n",
    "# Call the gemini model with the capability of calling functions and providing function call results\n",
    "def generate_gemini_completion(\n",
    "        user_prompt : str | None = None,\n",
    "        response_format: ResponseFormat | None = None,\n",
    "        system_prompt: str = DEFAULT_SYSTEM_PROMPT,\n",
    "        model_name: str = DEFAULT_MODEL, \n",
    "        function_declarations: list | None = None,\n",
    "        function_call_response: FunctionCallResponse | None = None,\n",
    "        verbose: bool = False\n",
    "        ): \n",
    "    \"\"\"\n",
    "    Call the GenAI model with function declarations and return the response.\n",
    "    Args:\n",
    "        user_prompt (str): The prompt to send to the model.\n",
    "        response_format (ResponseFormat): The format of the response.\n",
    "        system_prompt (str): The system prompt to use.\n",
    "        model_name (str): The name of the model to use.\n",
    "        function_declarations (list): List of function declarations.\n",
    "        verbose (bool): If True, print the response.\n",
    "    Returns:\n",
    "        GeminiResponse: The response from the model.\n",
    "    \"\"\"\n",
    "    global history\n",
    "\n",
    "    contents = history.copy()\n",
    "\n",
    "    # Add user_prompt to contents\n",
    "    if user_prompt:\n",
    "        # Append user prompt to history\n",
    "        user_content = types.Content(\n",
    "            role='user',\n",
    "            parts=[types.Part.from_text(text=user_prompt)]\n",
    "        )\n",
    "        contents.append(user_content)\n",
    "\n",
    "    # Add function call response to contents\n",
    "    if function_call_response:\n",
    "        function_response_part = types.Part.from_function_response(\n",
    "            name=function_call_response.name,\n",
    "            response={\"result\": function_call_response.result},\n",
    "        )\n",
    "        contents.append(types.Content(role=\"user\", parts=[function_response_part]))\n",
    "\n",
    "    # Configure tools\n",
    "    tools = []\n",
    "    if function_declarations:\n",
    "        tools.append(types.Tool(function_declarations=function_declarations))\n",
    "\n",
    "    # Configure response format\n",
    "    response_schema = None\n",
    "    response_mime_type = None\n",
    "    if response_format:\n",
    "        response_schema = response_format.response_schema\n",
    "        response_mime_type = response_format.response_mime_type.value\n",
    "\n",
    "    config = types.GenerateContentConfig(\n",
    "        tools=tools,\n",
    "        system_instruction=system_prompt,\n",
    "        response_schema=response_schema,\n",
    "        response_mime_type=response_mime_type,\n",
    "    )\n",
    "\n",
    "    # Send request with function declarations\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=contents,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    function_call = None\n",
    "    if response.candidates[0].content.parts[0].function_call:\n",
    "        function_call_gemini = response.candidates[0].content.parts[0].function_call\n",
    "        function_call = FunctionCall(\n",
    "            name=function_call_gemini.name,\n",
    "            arguments=function_call_gemini.args,\n",
    "        )\n",
    "        # Append the function call to the history\n",
    "        contents.append(types.Content(role=\"model\", parts=[types.Part(function_call=function_call_gemini)]))\n",
    "\n",
    "    # Append the model response to the history\n",
    "    model_response = response.candidates[0].content.parts[0].text\n",
    "    if model_response:\n",
    "            model_content = types.ModelContent(model_response)\n",
    "            contents.append(model_content)\n",
    "    \n",
    "    # Update the history\n",
    "    history += contents\n",
    "    if verbose:\n",
    "        print(f\"History: {history}\")\n",
    "    \n",
    "    return GeminiResponse(\n",
    "        text=model_response,\n",
    "        function_call=function_call,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8534f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle function calls\n",
    "def handle_function_call(response: GeminiResponse, user_prompt: str, system_prompt: str, function_declarations: list[dict], function_map: dict) -> tuple[str, GeminiResponse]:\n",
    "    \"\"\"\n",
    "    Handles function calls from the Gemini model response. If a function call is detected,\n",
    "    it calls the specified function with the provided arguments and sends back the result.\n",
    "\n",
    "    Args:\n",
    "        response: The response object from the Gemini model.\n",
    "        user_prompt: The user's original prompt.\n",
    "        system_prompt: The system's initial prompt.\n",
    "        function_declarations: List of function declarations for Gemini to reference.\n",
    "        function_map: A dictionary mapping function names to actual Python functions.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (response_text, final_response), where:\n",
    "        - response_text is the plain text response for the user.\n",
    "        - final_response is the complete response object from Gemini.\n",
    "    \"\"\"\n",
    "    if response.function_call:\n",
    "        func_name = response.function_call.name\n",
    "        func_args = response.function_call.arguments\n",
    "        \n",
    "        if func_name in function_map:\n",
    "            # Call the function dynamically\n",
    "            result = function_map[func_name](**func_args)\n",
    "            \n",
    "            # Prepare the response for the Gemini model\n",
    "            function_call_response = FunctionCallResponse(\n",
    "                name=func_name,\n",
    "                result=result,\n",
    "            )\n",
    "            final_response = generate_gemini_completion(\n",
    "                user_prompt=user_prompt,\n",
    "                system_prompt=system_prompt,\n",
    "                function_declarations=function_declarations,\n",
    "                function_call_response=function_call_response,\n",
    "            )\n",
    "            return final_response.text, final_response\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Function call not recognized: {func_name}\")\n",
    "    \n",
    "    # If no function call, just return the text\n",
    "    return response.text, response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e8ebf",
   "metadata": {},
   "source": [
    "##### Providing the tool\n",
    "In this section we define the available tools for the models. In order to provide tool capabilities to the model, we need function declarations (which is just text) and the actual implementation. The actual implementation is what we use to call the function ourself, if the models decides to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "72275931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual implementation of the function to be called. The responsibility to call the function is on our side (not gemini)\n",
    "def call_book_order_api(isbn: str):\n",
    "    \"\"\"\n",
    "    Call the book order API with the given ISBN number.\n",
    "    Args:\n",
    "        isbn (str): The ISBN number of the book to order.\n",
    "    Returns:\n",
    "        dict: The response from the API.\n",
    "    \"\"\"\n",
    "\n",
    "    if isbn == \"978-3-51593-12345-6\":\n",
    "        return {\"status\": \"success\", \"order_id\": \"123-FUNCTION-CALLING-456\"}\n",
    "    else:\n",
    "        return {\"status\": \"error\", \"message\": \"Invalid ISBN number.\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ac7ea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaration of the available function\n",
    "\n",
    "# Name of the function to be called. We need this later to map the declaration to the implementation.\n",
    "order_api_function_name = \"order_book\"\n",
    "\n",
    "# Function declaration for the book order API. This is what the model will see.\n",
    "call_book_order_api_declaration = {\n",
    "    \"name\": order_api_function_name,\n",
    "    \"description\": \"Order a book, which is provided by its ISBN number.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"isbn\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The ISBN number of the book.\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"isbn\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create a list of function declarations\n",
    "function_declarations = [call_book_order_api_declaration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3300889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping function names to actual functions. We use this for dynamic function calling.\n",
    "function_map = {\n",
    "    order_api_function_name: call_book_order_api,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "54c432ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The updated system prompt for the bookstore bot. We do not include information about the response format.\n",
    "# We tell the model to use tools.\n",
    "\n",
    "system_prompt = \"You are a friendly assistant working at a bookstore.\" \\\n",
    "    \"Your task is to help customers to order the book they are looking for.\" \\\n",
    "    \"If a customer does not know the title of the book, use your extensive knowledge to find information\" \\\n",
    "    \"Provide the title of the book, the year of publication and a short summary of the book\" \\\n",
    "    \"In addition, you should give two choices to the customer.\" \\\n",
    "    \"Choice 1: You order the book for the customer.\" \\\n",
    "    \"Choice 2: You provide a link to the ebook.\" \\\n",
    "    \"Formulate the choices in a sentence.\" \\\n",
    "    \"If the customer wants to provide a link to the ebook, you reply with the url of the ebook.\" \\\n",
    "    \"If the customer wants to order a book,  you use tools to order a book.\" \\\n",
    "    \"Use your extensive knowledge to find the isbn number of the book.\" \\\n",
    "    \"Do not provide the isbn number in the response.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee6588a",
   "metadata": {},
   "source": [
    "##### Interactive chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An interactive chat with the bookstore bot to show the dynamic function calling capabilities.\n",
    "def start_chat():\n",
    "  \n",
    "  # Clear the chat history\n",
    "  clear_history()\n",
    "\n",
    "  print(\"Type 'exit' to stop the chat.\\n\")\n",
    "  while True:\n",
    "      # 🗣️ Get user input\n",
    "      user_prompt = input(\"You: \")\n",
    "      \n",
    "      # 🚪 Exit condition\n",
    "      if user_prompt.lower() == \"exit\":\n",
    "          print(\"Exiting chat. Goodbye!\")\n",
    "          break\n",
    "\n",
    "      # 🔄 Generate response from the model\n",
    "      response = generate_gemini_completion(\n",
    "          user_prompt=user_prompt,\n",
    "          system_prompt=system_prompt,\n",
    "          function_declarations=function_declarations,\n",
    "      )\n",
    "\n",
    "      # 🤖 Handle potential function calls\n",
    "      response_text, _ = handle_function_call(\n",
    "          response,\n",
    "          user_prompt=user_prompt,\n",
    "          system_prompt=system_prompt,\n",
    "          function_declarations=function_declarations,\n",
    "          function_map=function_map,\n",
    "      )\n",
    "      \n",
    "      # 💬 Display the bot's response\n",
    "      print(f\"\\033[1mYou:\\033[0m {user_prompt}\\n\")\n",
    "      print(f\"\\033[1mBookstore Bot:\\033[0m {response_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5786da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's chat!\n",
    "# You can use this prompt to start a conversation with the bot: I'm looking for this book, where Sherlock Holmes and Watson meet the first time.\n",
    "start_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
