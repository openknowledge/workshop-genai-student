{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b938e36a3a85e97d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# GenAI Workshop\n",
    "## Lesson 2: Prompt Engineering\n",
    "\n",
    "This lesson is intended to improve your prompt engineering skills.\n",
    "\n",
    "During this lesson you will learn how to ...\n",
    "* use a system prompt to define model behaviour\n",
    "* extend system prompt to create workflows\n",
    "* specify output format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e600adda22789bde",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Set up the environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94c0f71909f6ae",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Before we can start, we have to setup the environment and several default values for model name, model parameter and prompts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeeafd26f6782f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:27:44.526792Z",
     "start_time": "2024-11-03T11:27:44.233725Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "import time\n",
    "from pathlib import Path\n",
    "from google.colab import userdata\n",
    "\n",
    "\n",
    "# Check runtime environment to make sure we are running in a colab environment. \n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   COLAB = True\n",
    "   print(\"Running on COLAB environment.\") \n",
    "else:\n",
    "   COLAB = False\n",
    "   print(\"WARNING: Running on LOCAL environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the data repository into colab\n",
    "!git clone https://github.com/openknowledge/workshop-genai-data.git\n",
    "PROCESSED_DATA_PATH = \"/content/workshop-genai-data/processed/gutenberg/\"\n",
    "BOOK_DB = PROCESSED_DATA_PATH + \"books.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a030be6602a51e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:27:45.922612Z",
     "start_time": "2024-11-03T11:27:45.913705Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Google GenAI Client API with GOOGLE_API_KEY to be able to call the model.\n",
    "# Note: GEMINI_API_KEY must be set as COLAB userdata before!\n",
    "GOOGLE_API_KEY=userdata.get('GEMINI_API_KEY')\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd7c56f49c1df",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Double check key settings by printing it out (or at least it length). \n",
    "if GOOGLE_API_KEY: \n",
    "    print(f' GOOGLE_API_KEY set with a length of {len(GOOGLE_API_KEY)}')\n",
    "else: \n",
    "    print(f' ERROR: GOOGLE_API_KEY not set correctly!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee82e5c07ded99",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Definition of convenient functions  \n",
    "\n",
    "The three following methods will simplify to work with the GEMINI genai model.\n",
    "For details see function documentation.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ec0540c47fd175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:27:47.628519Z",
     "start_time": "2024-11-03T11:27:47.622824Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set default values for model, model parameters and prompt\n",
    "DEFAULT_MODEL = \"gemini-2.0-flash\"\n",
    "DEFAULT_CONFIG_TEMPERATURE = 0.9 \n",
    "DEFAULT_CONFIG_TOP_K = 1\n",
    "DEFAULT_CONFIG_MAX_OUTPUT_TOKENS = 200 \n",
    "DEFAULT_SYSTEM_PROMPT = \"Your are a friendly assistant\"\n",
    "DEFAULT_USER_PROMPT:str = \" \"\n",
    "\n",
    "class MimeType(Enum):\n",
    "    \"\"\"\n",
    "    Enum for MIME types.\n",
    "    \"\"\"\n",
    "    JSON = \"application/json\"\n",
    "\n",
    "\n",
    "class ResponseFormat(BaseModel):\n",
    "    \"\"\"\n",
    "    Response format model for Gemini API.\n",
    "    \"\"\"\n",
    "    response_mime_type: MimeType\n",
    "    response_schema: type\n",
    "    \n",
    "history = [\n",
    "    types.Part.from_bytes(\n",
    "        data=Path(BOOK_DB).read_bytes(),\n",
    "        mime_type='text/plain',\n",
    "    )\n",
    "]\n",
    "\n",
    "def clear_history():\n",
    "    \"\"\"\n",
    "    Clear the history of the conversation.\n",
    "    \"\"\"\n",
    "    global history\n",
    "    history = [\n",
    "        types.Part.from_bytes(\n",
    "            data=Path(BOOK_DB).read_bytes(),\n",
    "            mime_type='text/plain',\n",
    "        )\n",
    "    ]\n",
    "\n",
    "def generate_bookstore_bot_completion(\n",
    "        user_prompt : str,\n",
    "        response_schema: BaseModel | None = None,\n",
    "        system_prompt: str = DEFAULT_SYSTEM_PROMPT,\n",
    "        model_name: str = DEFAULT_MODEL,\n",
    "        verbose: bool = False\n",
    "        ): \n",
    "    \"\"\"\n",
    "    Call the GenAI model with function declarations and return the response.\n",
    "    Args:\n",
    "        user_prompt (str): The prompt to send to the model.\n",
    "        response_format (ResponseFormat): The format of the response.\n",
    "        system_prompt (str): The system prompt to use.\n",
    "        model_name (str): The name of the model to use.\n",
    "        verbose (bool): If True, print the response.\n",
    "    Returns:\n",
    "        str: The response from the model.\n",
    "    \"\"\"\n",
    "    global history\n",
    "\n",
    "    # Append file content if provided\n",
    "    user_content = types.Content(\n",
    "        role='user',\n",
    "        parts=[types.Part.from_text(text=user_prompt)]\n",
    "    )\n",
    "\n",
    "    # Update history with user content\n",
    "    history.append(user_content)\n",
    "\n",
    "    # Configure response format\n",
    "    response_mime_type = None\n",
    "    if response_schema:\n",
    "        response_mime_type = MimeType.JSON.value\n",
    "\n",
    "    config = types.GenerateContentConfig(\n",
    "        system_instruction=system_prompt,\n",
    "        response_schema=response_schema,\n",
    "        response_mime_type=response_mime_type,\n",
    "    )\n",
    "\n",
    "    # Send request with function declarations\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=history,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # Update history with assistant content\n",
    "    bot_content = types.ModelContent(response.text)\n",
    "\n",
    "    history.append(user_content)\n",
    "    history.append(bot_content)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"User Prompt: {user_prompt}\")\n",
    "        print(f\"Assistant Response: {response.text}\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46449b2da08c3c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:27:48.272806Z",
     "start_time": "2024-11-03T11:27:48.270027Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def print_completion_result(completion_result, full:bool = False):\n",
    "    \n",
    "    \"\"\" Prints out the completion.    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    completion_result : str\n",
    "        A instance of GenerateContentResponse representing a completion \n",
    "    full : bool, optional [default: False]\n",
    "    Whether to print all details of the completion or only the text. Defaults to False\n",
    "    \n",
    "    \"\"\"            \n",
    "    print(f'\\nANSWER of genAI model: \\n')\n",
    "    if full:\n",
    "        print(completion_result)\n",
    "    else: \n",
    "        print(completion_result.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73227a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_book(isbn:str):\n",
    "    \"\"\" Orders a book by its ISBN number. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    isbn : str\n",
    "        The ISBN number of the book to order.\n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    order_status : str\n",
    "        The status of the order. \n",
    "    \"\"\"\n",
    "    # Simulate ordering the book\n",
    "    print(f\"Ordering book with ISBN: {isbn}\")\n",
    "    # Print \".\" every 0.5 seconds\n",
    "    for _ in range(5):\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        time.sleep(0.5)\n",
    "    print()\n",
    "    if isbn == \"978-3-51593-12345-6\":\n",
    "        print(\"Success: You ordered 'A Study in Scarlet'!\")\n",
    "        print(\"You completed this exercise successfully!\")\n",
    "    else:\n",
    "        print(\"Error: Unknown ISBN number.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c29727",
   "metadata": {},
   "source": [
    "### Important notice: History\n",
    "In this (and only this) exercise, we implemented a history feature, so that the conversation can keep on going. If you mess up, use the following function to clear the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9097d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the chat history\n",
    "clear_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14034189b3eeaf1b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Exercise 01: Basic interaction\n",
    "Your task is to create a simple chatbot, which is able to order a book based on the description of a book.  \n",
    "Imagine the following situation:\n",
    "A customer of a bookstore wants to buy a book, but does not remember its title.  \n",
    "The customer might ask an employee: *\"I'm looking for this book, where Sherlock Holmes and Watson meet the first time.\"*\n",
    "In response an employee will use his/her extensive knowledge of books and say: \"That's \"A Study in Scarlet\"! Should I place an order for this book?\"\n",
    "  \n",
    "The employee's reaction described here, should now be done by a chatbot. Your task is to provide a system prompt for this bot. \n",
    "The bot should:  \n",
    "* Respond to the customer by naming the book title, year of publication and a short summary of the book.\n",
    "* In addition the bot should ask, if the book should be ordered.  \n",
    "\n",
    "**Hints**: The creators of the bot (aka teacher of this workshop) already provided knowledge about books to the bot. You do not need to worry about this.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd0114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change the user prompt.\n",
    "user_prompt = \"I'm looking for this book, where Sherlock Holmes and Watson meet the first time.\"\n",
    "\n",
    "# TODO: Define the respective system prompt.\n",
    "system_prompt = \"TODO\"\n",
    "\n",
    "# We provide some knowledge to the model about the book contents.\n",
    "response = generate_bookstore_bot_completion(user_prompt=user_prompt,system_prompt=system_prompt)\n",
    "print_completion_result(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174613ed",
   "metadata": {},
   "source": [
    "### Exercise 02: Extend the process\n",
    "Extend the system prompt even further.\n",
    "After the bot provided the information about the book and asks if it should be ordered, the *customer might say*: \"Yes, please!\" As an result, the bot will order the book. This process is finished by the bot replying with an object like {\"isbn\": \"978-4-23050-12345-6\"}. This object represents the payload for ordering the book using an API.\n",
    "Your task is to extend the system prompt in order to achieve this behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be1bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the respective system prompt.\n",
    "system_prompt = \"TODO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52fdbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the chat history\n",
    "clear_history()\n",
    "\n",
    "# The customer is looking for a book.\n",
    "customer_initial_prompt = \"I'm looking for this book, where Sherlock Holmes and Watson meet the first time.\"\n",
    "print(f'Customer:\\n {customer_initial_prompt}\\n')\n",
    "\n",
    "# The bookstore bot answers the customer.\n",
    "response = generate_bookstore_bot_completion(user_prompt=customer_initial_prompt, system_prompt=system_prompt)\n",
    "print(f'Bookstore bot:\\n {response.text}\\n')\n",
    "\n",
    "# The customer wants to order the book.\n",
    "customer_answer = \"Yes, I like to order the book.\"\n",
    "print(f'Customer:\\n {customer_answer}\\n')\n",
    "\n",
    "# The bookstore bot answers the customer.\n",
    "response = generate_bookstore_bot_completion(user_prompt=customer_answer, system_prompt=system_prompt)\n",
    "print(f'Bookstore bot:\\n {response.text}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Order the book by using the ISBN number provided by the bot.\n",
    "isbn = \"TODO\"\n",
    "order_book(isbn=isbn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d501ff",
   "metadata": {},
   "source": [
    "### Exercise 03: Give choices\n",
    "After the initial customer request, the bot should give the user another choice. Instead of ordering the book, the bot can provide an url to the ebook version of the book. Update the system prompt in order to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa19f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Update the system prompt to include the option to provide an url to the ebook instead of ordering the book.\n",
    "system_prompt = \"TODO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fd240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the chat history\n",
    "clear_history()\n",
    "\n",
    "# The customer is looking for a book.\n",
    "customer_initial_prompt = \"I'm looking for this book, where Sherlock Holmes and Watson meet the first time.\"\n",
    "print(f'Customer:\\n {customer_initial_prompt}\\n')\n",
    "\n",
    "# The bookstore bot answers the customer.\n",
    "response = generate_bookstore_bot_completion(user_prompt=customer_initial_prompt, system_prompt=system_prompt)\n",
    "print(f'Bookstore bot:\\n {response.text}\\n')\n",
    "\n",
    "# The customer wants to to have the link to the ebook.\n",
    "customer_answer = \"I love ebooks. Why should I order a book, if I get an ebook for free. Please provide the link!\"\n",
    "print(f'Customer:\\n {customer_answer}\\n')\n",
    "\n",
    "# The bookstore bot answers the customer.\n",
    "response = generate_bookstore_bot_completion(user_prompt=customer_answer, system_prompt=system_prompt)\n",
    "print(f'Bookstore bot:\\n {response.text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the old process of ordering a book still works\n",
    "\n",
    "# Clear the chat history\n",
    "clear_history()\n",
    "\n",
    "# The customer is looking for a book.\n",
    "customer_initial_prompt = \"I'm looking for this book, where Sherlock Holmes and Watson meet the first time.\"\n",
    "print(f'Customer:\\n {customer_initial_prompt}\\n')\n",
    "\n",
    "# The bookstore bot answers the customer.\n",
    "response = generate_bookstore_bot_completion(user_prompt=customer_initial_prompt, system_prompt=system_prompt)\n",
    "print(f'Bookstore bot:\\n {response.text}\\n')\n",
    "\n",
    "# The customer wants to order the book.\n",
    "customer_answer = \"Please order the book. I love spending money for stuff I can get for free!\"\n",
    "print(f'Customer:\\n {customer_answer}\\n')\n",
    "\n",
    "# The bookstore bot answers the customer.\n",
    "response = generate_bookstore_bot_completion(user_prompt=customer_answer, system_prompt=system_prompt)\n",
    "print(f'Bookstore bot:\\n {response.text}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
