{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1b8b6742cff65fee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GenAI Workshop\n",
    "## Lesson 2: GenAI Starter \n",
    "\n",
    "This lesson is intended to play around with prompting and model parameter settings. \n",
    "\n",
    "During this lesson you will learn how to ...\n",
    "\n",
    "- use diverse roles for prompting\n",
    "- apply different prompting patterns \n",
    "- manipulate the model completion via model parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b938e36a3a85e97d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set up the environment "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e600adda22789bde"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we can start, we have to setup the environment and several default values for model name, model parameter and prompts.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b94c0f71909f6ae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai   \n",
    "\n",
    "# Check runtime environment to make sure we are running in a colab environment. \n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   COLAB = True\n",
    "   print(\"Running on COLAB environment.\") \n",
    "else:\n",
    "   COLAB = False\n",
    "   print(\"WARNING: Running on LOCAL environment.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbeeafd26f6782f5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import colab specific lib to read user data (aka colab managed secrets)\n",
    "from google.colab import userdata"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e46c42368eef73ef"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize Google GenAI Client API with GOOGLE_API_KEY to be able to call the model. \n",
    "# Note: GEMINI_API_KEY must be set as COLAB userdata before! \n",
    "GOOGLE_API_KEY=userdata.get('GEMINI_API_KEY')  \n",
    "genai.configure(api_key=GOOGLE_API_KEY)  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e292ed72b7fa718",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Double check key settings by printing it out (or at least it length). \n",
    "if GOOGLE_API_KEY: \n",
    "    print(f' GOOGLE_API_KEY set with a length of {len(GOOGLE_API_KEY)}')\n",
    "else: \n",
    "    print(f' ERROR: GOOGLE_API_KEY not set correctly!')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82c9941319bfa9af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of convenient functions  \n",
    "\n",
    "The three following methods will simplify to work with the GEMINI genai model.\n",
    "For details see function documentation.   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bee82e5c07ded99"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO take a look at the following convenient functions and try to understand\n",
    "#\n",
    "# call_genai_model_for_completion: Encapsulates the google gemini genai call.\n",
    "# print_completion_result: Prints out google gemini genai call result.\n",
    "# extract_text_from_pdf: Extracts text from a pdf file. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d845da4e9e4530b6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# set default values for model, model parameters and prompt\n",
    "DEFAULT_MODEL = \"gemini-1.5-flash\"\n",
    "DEFAULT_CONFIG_TEMPERATURE = 0.9 \n",
    "DEFAULT_CONFIG_TOP_K = 1\n",
    "DEFAULT_CONFIG_MAX_OUTPUT_TOKENS = 200 \n",
    "DEFAULT_SYSTEM_PROMPT = \"Your are a friendly assistant\"\n",
    "DEFAULT_USER_PROMPT:str = \" \"\n",
    "\n",
    "def call_genai_model_for_completion(\n",
    "        model_name: str = DEFAULT_MODEL, \n",
    "        temperature:float = DEFAULT_CONFIG_TEMPERATURE,\n",
    "        top_k: int = DEFAULT_CONFIG_TOP_K, \n",
    "        max_output_tokens: int = DEFAULT_CONFIG_MAX_OUTPUT_TOKENS, \n",
    "        system_prompt : str = DEFAULT_SYSTEM_PROMPT, \n",
    "        user_prompt : str = DEFAULT_USER_PROMPT,\n",
    "        file_list : [str] = None, \n",
    "        verbose: bool = False\n",
    "        ): \n",
    "    \n",
    "    \n",
    "    \"\"\" Calls a gemini model with a given set of parameters and returns the completions \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str, optional [default: DEFAULT_MODEL]\n",
    "        The name of the model to use for the completion\n",
    "    temperature : float, optional [default: DEFAULT_CONFIG_TEMPERATURE]\n",
    "        The temperature of the model\n",
    "    top_k : int, optional [default: DEFAULT_CONFIG_TOP_K]\n",
    "        The number of most recent matches to return\n",
    "    max_output_tokens : int, optional [default: DEFAULT_CONFIG_MAX_OUTPUT_TOKENS]\n",
    "        The maximum number of output tokens to return\n",
    "    system_prompt : str, optional [default: DEFAULT_SYSTEM_PROMPT]\n",
    "        The system prompt to use for the completion\n",
    "    user_prompt : str, optional [default: DEFAULT_USER_PROMPT]\n",
    "        The user prompt to use for the completion\n",
    "    file_list : [str], optional [default: empty list]\n",
    "    verbose : bool, optional [default: False]\n",
    "        Whether to print details of the completion process or not. Defaults to False   \n",
    "         \n",
    "    Returns \n",
    "    -------\n",
    "    completions :\n",
    "        a GenerateContentResponse instance representing the genAI model answer(s)       \n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose: \n",
    "        # print out summary of input values / parameters\n",
    "        print(f'Generating answer for following config:')\n",
    "        print(f'  - SYSTEM PROMPT used:\\n {system_prompt}')\n",
    "        print(f'  - USER PROMPT used:\\n {user_prompt}')\n",
    "        print(f'  - MODEL used:\\n {model_name} (temperature = {temperature}, top_k = {top_k}, max_output_tokens = {max_output_tokens})')\n",
    "\n",
    "    # create generation config \n",
    "    model_config = genai.GenerationConfig(\n",
    "        max_output_tokens=max_output_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    \n",
    "    # create genai model with generation config \n",
    "    genai_model = genai.GenerativeModel(\n",
    "        model_name= model_name,\n",
    "        system_instruction= system_prompt, \n",
    "        generation_config= model_config\n",
    "    )\n",
    "    \n",
    "    if file_list: \n",
    "        contents = [user_prompt] + file_list\n",
    "    else: \n",
    "        contents = user_prompt\n",
    "    \n",
    "    response = genai_model.generate_content(contents)\n",
    "    return response; "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8ec0540c47fd175",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def print_completion_result(completion_result, full:bool = False):\n",
    "    \n",
    "    \"\"\" Prints out the completion.    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    completion_result : str\n",
    "        A instance of GenerateContentResponse representing a completion \n",
    "    full : bool, optional [default: False]\n",
    "    Whether to print all details of the completion or only the text. Defaults to False\n",
    "    \n",
    "    \"\"\"    \n",
    "        \n",
    "    print(f'\\nANSWER of genAI model: \\n')\n",
    "    if full:\n",
    "        print(completion_result)\n",
    "    else: \n",
    "        print(completion_result.text) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f46449b2da08c3c5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%pip install PyPDF2\n",
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \n",
    "    \"\"\" Extract text from a pdf file and return     \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pdf_path : str\n",
    "        full qualified path name of the pdf file     \n",
    "\n",
    "    Returns \n",
    "    -------\n",
    "    extracted_text :\n",
    "        The extracted text from the pdf file      \n",
    "    \"\"\"\n",
    "    \n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        extracted_text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                extracted_text += text\n",
    "        return extracted_text\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4a5f3afdcc39d35"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 01: Prompting with roles \n",
    "\n",
    "During this exercise you will how to use the different types of prompts.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14034189b3eeaf1b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO Call genai model for completion with \n",
    "# - step 1: no prompt at all\n",
    "# - step 2: user prompt only \n",
    "# - step 3: user prompt and system prompt \n",
    "#\n",
    "# Calling the genai model with user prompt and system prompt \n",
    "# should result in prefering a german city "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4ffbee18fced674"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "user_prompt = \"What is the most beautiful city in the world?\"\n",
    "system_prompt = \"You are a friendly assistant.\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58a4e3165c2e2d44"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 1: call genai model without any prompts\n",
    "\n",
    "# TODO \n",
    "#  Call genai model for completion without any prompt. \n",
    "#  Print completion result.  \n",
    "response = None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99ba098493020806",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 2: call genai model with user prompt only\n",
    "\n",
    "# TODO \n",
    "#  Call genai model for completion wit user prompt only. \n",
    "#  Print completion result.  \n",
    "response = None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee190ebf1f6260c5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 3: call genai model with system and user prompt\n",
    "# TODO \n",
    "#  Call genai model for completion wit user prompt and system prompt. \n",
    "#  Print completion result.  \n",
    "response = None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89e423141c25fc76",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 02: Prompting patterns and best practices\n",
    "\n",
    "In this exercise, you will learn how to apply various prompting best practices to achieve the desired result. See [Prompt Engineering Guide](https://www.promptingguide.ai/techniques) for more information. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dcd0c8b9529083b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Prompting parts \n",
    "\n",
    "To obtain the desired result from the genai model when prompting, try to take the following prompt proportions into account:  \n",
    "\n",
    "- role: \"Who am I, while 'talking' to the model?\"\n",
    "- context: \"Are there any additional information that can help the model to answer my question?\"\n",
    "- question: \"What is the task/action/question I ask for?\" \n",
    "- output: \"What kind of output (format) do I expect?\"\n",
    "- example(s): \"Are there any helpful examples the model can use?\"  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "207448e7dd2eac5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "initial_prompt = \"I want to go on holiday. Where should I go?\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-01T16:16:39.033922Z",
     "start_time": "2024-11-01T16:16:39.028864Z"
    }
   },
   "id": "a1b9ede126b90994",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Call genai model for completion with initial prompt.  \n",
    "response = call_genai_model_for_completion(user_prompt=initial_prompt)\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b37ac0a679611c8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO Create a better prompt following the 'prompting parts' best practices. \n",
    "prompt_role = \"YOUR ROLE PART HERE\"\n",
    "prompt_context = \"YOUR CONTEXT PART HERE\"\n",
    "prompt_question = \"YOUR QUESTION PART HERE \"\n",
    "prompt_output = \"YOUR OUTPUT PART HERE \""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a79e316124f8c01"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt_with_parts = f'{prompt_role} {prompt_context} {prompt_question} {prompt_output}'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c4cf823de2bf085"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Call genai model for completion with prompt with parts.  \n",
    "response = call_genai_model_for_completion(user_prompt=prompt_with_parts)\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6824faba13a5d66b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO \n",
    "#  Use system prompt to define the systems role in detail in addition \n",
    "#  to get an even better or more specific result. \n",
    "#  - personality: travel web site Wonder-World AI chatbot\n",
    "#  - name: Wonder-World (always greet with your name ;-) )\n",
    "#  - mission:  provide helpful queries for travelers.\n",
    "#  - guardrails: \n",
    "#       - answer with advise only if question complies with mission \n",
    "#       - else say \"Sorry I can't answer that question\"  \n",
    "system_prompt = \"You are a friendly assistant\"        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T11:12:58.969850Z",
     "start_time": "2024-11-03T11:12:58.965287Z"
    }
   },
   "id": "d7fbc8c0dcba241c",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Call genai model for completion with prompt with parts.  \n",
    "response = call_genai_model_for_completion(user_prompt=prompt_with_parts, system_prompt = system_prompt)\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16d639f13ffcb0b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Chain of Thoughts \n",
    "\n",
    "Introduced in Wei et al. (2022), chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b7286d90b642357"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise we want the genai model to determine if our statement is true or false."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "365ffa3edfa38177"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# This is the statement we want to check (as false) \n",
    "statement_to_evaluate = \"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4b97ae16dbc4122"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO \n",
    "#  Build up a chain of thoughts to help genai to create the right answer: \n",
    "#  \n",
    "#  Use\n",
    "#\n",
    "#   - The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1. \n",
    "#  \n",
    "#  and a corresponding explanation why this is false to help the genai model to answer \n",
    "#  the following statement correctly: \n",
    "#   \n",
    "#   - The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
    "\n",
    "chain_of_thought = \"\"\n",
    " \n",
    "chain_of_thought_prompt = (\n",
    "    chain_of_thought + \n",
    "    statement_to_evaluate + \n",
    "    \"Answer:\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b18e1f02b4e83c4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Call genai model for completion with chain of thought prompt as user prompt.  \n",
    "response = call_genai_model_for_completion(user_prompt=chain_of_thought_prompt)\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c268b37229548273"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Few Shot Learning\n",
    "\n",
    "While large-language models demonstrate remarkable zero-shot capabilities, they still fall short on more complex tasks when using the zero-shot setting. Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a782178c771a88b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise we want the genai model to rate a given sentence as positive or negative.\n",
    "Use few shot learning to support the genai model. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c6521ad849bc695"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# This is the text to rate (as negative)\n",
    "text_to_rate = \"What a horrible show!\" "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T11:38:51.690629Z",
     "start_time": "2024-11-03T11:38:51.683214Z"
    }
   },
   "id": "a9f194dbd0f0b0f9",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO\n",
    "#   Build up a few shot prompt that helps the genai model to determine if a given  \n",
    "#   statement is meant positive / negative.\n",
    "few_shot_prompt = (\"YOUR EXAMPLE(S) // RATING(S) HERE \\n\"\n",
    "          + text_to_rate + \" // \\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-03T11:38:52.014454Z",
     "start_time": "2024-11-03T11:38:52.007469Z"
    }
   },
   "id": "19f1b394d8d81077",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Call genai model for completion with few shot prompt as user prompt.\n",
    "response = call_genai_model_for_completion(user_prompt=few_shot_prompt)\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e66833251dfa71c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ### Exercise 03: Models and parameters\n",
    "\n",
    "In this exercise, you will learn how to use the various genai model parameters to customise the result according to your wishes. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dc44c1ab22b3056"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# You want to know why the color of the sky is blue.\n",
    "model_parameter_prompt = \"Why is the sky blue?\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd81467de62fe661"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: Play around with different model parameter to get various answers  \n",
    "\n",
    "temperature = 0.5\n",
    "top_k = 10 \n",
    "max_output_tokens = 200"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec797c2ad1e3d020"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Call genai model for completion with few shot prompt as user prompt.\n",
    "response = call_genai_model_for_completion(user_prompt=model_parameter_prompt, temperature=temperature, top_k = top_k, max_output_tokens = max_output_tokens)\n",
    "print_completion_result(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "353035766a95a4b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 04: Augmenting \n",
    "\n",
    "In this exercise, you will learn how to use your own documents for augmenting the user prompt. To achieve this, we will first read in the files and then transfer their content to the GenAI model. \n",
    "\n",
    "To make this exercise work, you first have to upload the two files named \"the-mystery-house-001.pdf\" and \"the-mystery-house-002.pdf\" from the data/pdf folder to the colab sample-data folder.  \n",
    "\n",
    "NOTE: The above-mentioned mechanism works well for content / files smaller tha 20 MB. For larger content you should upload the content to the gemini model in advance. See [genai.upload_file](https://ai.google.dev/gemini-api/docs/document-processing) for detailed information. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "377d0d387e5c7ac8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO \n",
    "#   Ask questions about the two short stories (see data/pdf). \n",
    "# \n",
    "#   Play around with the number of files, you provide and try to \n",
    "#   understand the differences of the answers. \n",
    "\n",
    "# Define location of augmenting files. \n",
    "PDF_PATH_1 = \"the-mystery-house-001.pdf\"; \n",
    "PDF_PATH_2 = \"the-mystery-house-002.pdf\";\n",
    "\n",
    "# Extract text from augmenting files\n",
    "# This is a suitable solution for files smaller 20 MB. \n",
    "pdf_file_1 = extract_text_from_pdf(PDF_PATH_1)\n",
    "pdf_file_2 = extract_text_from_pdf(PDF_PATH_2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc8bb0361401f655"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define prompt with question about file content \n",
    "augmenting_prompt = \"Who is the main character of the story and what is his nickname?\"\n",
    "\n",
    "# call genai model with prompt and files \n",
    "response = call_genai_model_for_completion(user_prompt=augmenting_prompt, file_list=[pdf_file_1, pdf_file_2])\n",
    "print_completion_result(response)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8adc4707553696f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
