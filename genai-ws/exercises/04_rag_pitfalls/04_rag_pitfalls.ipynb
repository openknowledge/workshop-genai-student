{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GenAI Workshop\n",
    "## Lesson 5: RAG Pitfalls\n",
    "\n",
    "This lesson is intended to show you how different RAG configurations affect the output quality of a Retrieval Augmented Generation system.\n",
    "\n",
    "During this lesson you will learn how to ...\n",
    "\n",
    "- evaluate the quality of a RAG system using simple quality metrics\n",
    "- manipulate the *chunk size* to improve the quality metrics\n",
    "- manipulate *top k* to improve the quality metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxQu4eGGDrJe"
   },
   "source": [
    "### Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J595XWK4AuQ-",
    "outputId": "82527bf2-eb5b-4a12-fe2f-fdfc6f2df1ff"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   COLAB = True\n",
    "   print(\"Running on COLAB environment.\")\n",
    "else:\n",
    "   COLAB = False\n",
    "   print(\"WARNING: Running on LOCAL environment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ilgxHbeTwiJb",
    "outputId": "12db3911-fd8d-4d49-e64c-9ef960f61ec3"
   },
   "outputs": [],
   "source": [
    "# Clone the data repository into colab\n",
    "!git clone https://github.com/openknowledge/workshop-genai-data.git\n",
    "PROCESSED_DATA_PATH = \"/content/workshop-genai-data/processed/gutenberg/\"\n",
    "EVALUATION_DATA_PATH = \"/content/workshop-genai-data/evaluation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z06SMoItD5kL"
   },
   "outputs": [],
   "source": [
    "# import colab specific lib to read user data (aka colab managed secrets)\n",
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DqbDZeWD6nS"
   },
   "outputs": [],
   "source": [
    "# Initialize Google GenAI Client API with GOOGLE_API_KEY to be able to call the model.\n",
    "# Note: GEMINI_API_KEY must be set as COLAB userdata before!\n",
    "GOOGLE_API_KEY=userdata.get('GEMINI_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "OFEsl___H2n4"
   },
   "outputs": [],
   "source": [
    "# Install additional libraries\n",
    "%%capture\n",
    "!pip install -qU langchain-text-splitters\n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3UN1LX-IEcp"
   },
   "outputs": [],
   "source": [
    "# Import additional libraries\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from chromadb import EphemeralClient\n",
    "import requests\n",
    "import re\n",
    "import uuid\n",
    "import json\n",
    "import typing_extensions as typing\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXY2UNBCBmDQ"
   },
   "outputs": [],
   "source": [
    "# Configure pandas display options\n",
    "pd.set_option(\"max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKG1EPEGdvvC"
   },
   "outputs": [],
   "source": [
    "# Set default values for model, model parameters and prompt\n",
    "DEFAULT_MODEL = \"gemini-1.5-flash\"\n",
    "DEFAULT_CONFIG_TEMPERATURE = 0.0\n",
    "DEFAULT_CONFIG_TOP_K = 1\n",
    "DEFAULT_CONFIG_MAX_OUTPUT_TOKENS = 200\n",
    "DEFAULT_SYSTEM_PROMPT = \"Your are a friendly assistant\"\n",
    "DEFAULT_USER_PROMPT = \" \"\n",
    "\n",
    "# Set defaults for retrieval\n",
    "DEFAULT_TOP_K = 3\n",
    "DEFAULT_CHUNK_OVERLAP = 100\n",
    "DEFAULT_CHUNK_SIZE = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "CJW72wGgPOxt"
   },
   "outputs": [],
   "source": [
    "# This will be the chromadb collection we use as a knowledge base. We do not need the in-memory EphemeralClient.\n",
    "chromadb_client = EphemeralClient()\n",
    "chromadb_collection = chromadb_client.create_collection(name=\"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4s8AAOATdqCa"
   },
   "outputs": [],
   "source": [
    "def call_genai_model_for_completion(\n",
    "    model_name: str = DEFAULT_MODEL,\n",
    "    config_temperature:float = DEFAULT_CONFIG_TEMPERATURE,\n",
    "    config_top_k: int = DEFAULT_CONFIG_TOP_K,\n",
    "    config_max_output_tokens: int = DEFAULT_CONFIG_MAX_OUTPUT_TOKENS,\n",
    "    system_prompt : str = DEFAULT_SYSTEM_PROMPT,\n",
    "    user_prompt : str = DEFAULT_USER_PROMPT,\n",
    "    verbose: bool = False\n",
    "    ):\n",
    "    \"\"\" Calls a gemini model with a given set of parameters and returns the completions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str, optional [default: DEFAULT_MODEL]\n",
    "        The name of the model to use for the completion\n",
    "    temperature : float, optional [default: DEFAULT_CONFIG_TEMPERATURE]\n",
    "        The temperature of the model\n",
    "    top_k : int, optional [default: DEFAULT_CONFIG_TOP_K]\n",
    "        The number of most recent matches to return\n",
    "    max_output_tokens : int, optional [default: DEFAULT_CONFIG_MAX_OUTPUT_TOKENS]\n",
    "        The maximum number of output tokens to return\n",
    "    system_prompt : str, optional [default: DEFAULT_SYSTEM_PROMPT]\n",
    "        The system prompt to use for the completion\n",
    "    user_prompt : str, optional [default: DEFAULT_USER_PROMPT]\n",
    "        The user prompt to use for the completion\n",
    "    file_list : [str], optional [default: empty list]\n",
    "    verbose : bool, optional [default: False]\n",
    "        Whether to print details of the completion process or not. Defaults to False\n",
    "    Returns\n",
    "    -------\n",
    "    completions :\n",
    "        a GenerateContentResponse instance representing the genAI model answer(s)\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        # print out summary of input values / parameters\n",
    "        print(f'Generating answer for following config:')\n",
    "        print(f'  - SYSTEM PROMPT used:\\n {system_prompt}')\n",
    "        print(f'  - USER PROMPT used:\\n {user_prompt}')\n",
    "        print(f'  - MODEL used:\\n {model_name} (temperature = {config_temperature}, top_k = {config_top_k}, max_output_tokens = {config_max_output_tokens})')\n",
    "\n",
    "    # create generation config\n",
    "    model_config = genai.GenerationConfig(\n",
    "        max_output_tokens=config_max_output_tokens,\n",
    "        temperature=config_temperature,\n",
    "        top_k=config_top_k\n",
    "    )\n",
    "\n",
    "    # create genai model with generation config\n",
    "    genai_model = genai.GenerativeModel(\n",
    "        model_name= model_name,\n",
    "        generation_config= model_config\n",
    "    )\n",
    "\n",
    "    # Attention: We manipulated the safety settings\n",
    "    response = genai_model.generate_content(\n",
    "        contents=[system_prompt, user_prompt], safety_settings={\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    })\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bqI0tu1xfKaR"
   },
   "outputs": [],
   "source": [
    "# RAG building blocks\n",
    "\n",
    "# Get content of books. The content will already be cleansed.\n",
    "def load_file_content(file_name: str) -> str:\n",
    "  \"\"\" Loads content of a file in the local file systemby a given file name and return its content\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        The name of the file to be loaded\n",
    "    Returns\n",
    "    -------\n",
    "    file_content : str\n",
    "        the content of the file as a string\n",
    "  \"\"\"\n",
    "  with open(f\"{PROCESSED_DATA_PATH}{file_name}\", \"r\") as f:\n",
    "    return f.read()\n",
    "\n",
    "# Building Block \"Chunking\": Split the content into smaller chunks\n",
    "def do_chunk(text: str, chunk_size: int = DEFAULT_CHUNK_SIZE, chunk_overlap: int = DEFAULT_CHUNK_OVERLAP) -> list[str]:\n",
    "  \"\"\" Chunks a given text by a given chunk size and chunk overlap and returns a list of chunks\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "      The text to be chunked\n",
    "  chunk_size : int, optional [default: DEFAULT_CHUNK_SIZE]\n",
    "        The desired chunk size\n",
    "  chunk_overlap : int, optional [default: DEFAULT_CHUNK_OVERLAP]\n",
    "        The desired chunk overlap\n",
    "  Returns\n",
    "  -------\n",
    "  chunks: [str]\n",
    "      The created chunks\n",
    "  \"\"\"\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "      chunk_size=chunk_size,\n",
    "      chunk_overlap=chunk_overlap,\n",
    "      length_function=len,\n",
    "  )\n",
    "  return text_splitter.split_text(text=text)\n",
    "\n",
    "# Building Block \"Embedding\": Create multi dimensional embeddings for a given chunk.\n",
    "def do_embed(chunk: str) -> list[float]:\n",
    "  \"\"\" Embeds a given chunk and returns the embedding\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  chunk : str\n",
    "      The chunk to be embedded\n",
    "  Returns\n",
    "  -------\n",
    "  embedding: [float]\n",
    "      The created embedding\n",
    "  \"\"\"\n",
    "  return genai.embed_content(model=EMBEDDING_MODEL, content=chunk).get(\"embedding\")\n",
    "\n",
    "def do_batch_embed(chunks: list[str]) -> list[list[float]]:\n",
    "  \"\"\" Embeds a list of chunks and returns the embeddings\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  chunks : [str]\n",
    "      The chunks to be embedded\n",
    "  Returns\n",
    "  -------\n",
    "  embeddings: [list[float]]\n",
    "      The created embeddings\n",
    "  \"\"\"\n",
    "  return genai.embed_content(model=EMBEDDING_MODEL, content=chunks).get(\"embedding\")\n",
    "\n",
    "# Building Block \"Knowledgebase\": Store embeddings and the corresponding content in a vectorstore\n",
    "def persist_embeddings(chunks: list[str], embeddings: list[float])-> None:\n",
    "  \"\"\" Persists the embeddings and the chunks in the knowledgebase\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  chunks : [str]\n",
    "      The chunks to be persisted\n",
    "  embeddings: [list[float]]\n",
    "      The corresponding embeddings to be persisted\n",
    "  \"\"\"\n",
    "  chromadb_collection = chromadb_client.get_or_create_collection(name=\"default\")\n",
    "  # Persist the embeddings and the chunks in the knowledgebase\n",
    "  ids = [str(uuid.uuid4()) for _ in chunks]\n",
    "  chromadb_collection.add(ids=ids, documents=chunks, embeddings=embeddings)\n",
    "\n",
    "# Building Block \"Augmentation\": Create an updated prompt by merging the original user input with the provided context\n",
    "# Attention: We manipulated the augmented prompt in order to see the guardrails in action\n",
    "def augment(user_input: str, context: list[str]) -> str:\n",
    "  \"\"\" Augments a given user input by merging it with the provided context and returns the augmented prompt\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be augmented\n",
    "  context : [str]\n",
    "      The context to be merged with the user input\n",
    "  Returns\n",
    "  -------\n",
    "  augmented_prompt: str\n",
    "      The created augmented prompt\n",
    "  \"\"\"\n",
    "  prepared_context = \"\\n\".join(context)\n",
    "  augmented_prompt = f\"\"\"\n",
    "    Answer the question as detailed as possible from the provided context. If the answer is not in\n",
    "    provided context just say, 'answer is not available in the context', don't provide the wrong answer.\n",
    "    Respond short and concisely.\n",
    "    Context:\\n{prepared_context}?\\n\n",
    "    Question: \\n{user_input}\\n\n",
    "\n",
    "    Answer:\n",
    "  \"\"\"\n",
    "  return augmented_prompt\n",
    "\n",
    "# Building Block \"Top-k Fetching\": Get the k semantically closest chunks to the user input from the knowledgebase\n",
    "def do_top_k_fetching(user_input_embedding: list[float], top_k: int) -> tuple[list[str],list[float]]:\n",
    "  \"\"\" Fetches the k semantically closest chunks to the user input from the knowledgebase\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input_embedding : [float]\n",
    "      The embedding of the user input\n",
    "  top_k : int\n",
    "      The number of semantically closest chunks to be fetched\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  distances: [float]\n",
    "      The corresponding distances to the user_input_embedding\n",
    "  \"\"\"\n",
    "  # Since we will do the fetching always only for one user_input,\n",
    "  # instead of querying for multiple embeddings simultanously as allowed by the choma API,\n",
    "  # we add the embeddings below to a list and return only the first document (chunk)\n",
    "  chromadb_collection = chromadb_client.get_or_create_collection(name=\"default\")\n",
    "  results = chromadb_collection.query(query_embeddings=[user_input_embedding], n_results=top_k)\n",
    "  return (results[\"documents\"][0], results[\"distances\"][0]) # Return the distances to get better insights\n",
    "\n",
    "# Building Block \"Generation\": Use the generation model to create a response\n",
    "# We return the total token count for this exercise\n",
    "def generate_response(prompt: str) -> tuple[str, int]:\n",
    "  \"\"\" Generates a response for a given prompt\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  prompt : str\n",
    "      The prompt to be used for the generation\n",
    "  Returns\n",
    "  -------\n",
    "  response: str\n",
    "      The generated response\n",
    "  total_token_count: int\n",
    "      The total token count for the generation\n",
    "  \"\"\"\n",
    "  completion_result = call_genai_model_for_completion(\n",
    "      model_name=GENERATION_MODEL,\n",
    "      user_prompt=prompt,\n",
    "  )\n",
    "  total_token_count = completion_result.usage_metadata.total_token_count\n",
    "  return (completion_result.text, total_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2W1KNEU7fKaR"
   },
   "outputs": [],
   "source": [
    "def do_ingestion(file_names: list[str], chunk_size: int = DEFAULT_CHUNK_SIZE, clear_knowledgebase: bool = False, verbose: bool = False) -> None:\n",
    "  \"\"\" Ingests a list of files by a given file name and chunk\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  file_names : [str]\n",
    "      The names of the files to be ingested\n",
    "  chunk_size : int, optional [default: DEFAULT_CHUNK_SIZE]\n",
    "      The desired chunk size\n",
    "  clear_knowledgebase : bool, optional [default: False]\n",
    "      Whether to clear the knowledgebase before ingesting the new files\n",
    "  verbose : bool, optional [default: False]\n",
    "      Whether to print details of the ingestion process or not. Defaults to False\n",
    "  \"\"\"\n",
    "  if clear_knowledgebase:\n",
    "    chromadb_client.delete_collection(name=\"default\")\n",
    "  # Ingest file by file\n",
    "  for file_name in file_names:\n",
    "    # Load prepared book content\n",
    "    file_content = load_file_content(file_name)\n",
    "\n",
    "    # Chunk the content into smaller chunks\n",
    "    chunks = do_chunk(file_content, chunk_size=chunk_size)\n",
    "    if verbose:\n",
    "      print(f'Loaded {len(chunks)} chunks from {file_name}')\n",
    "\n",
    "    # Embed the chunks\n",
    "    embeddings = do_batch_embed(chunks)\n",
    "\n",
    "    # Persist the embeddings and the chunks in the knowledgebase\n",
    "    persist_embeddings(chunks, embeddings)\n",
    "\n",
    "  if verbose:\n",
    "    chunks_count = chromadb_client.get_collection(name=\"default\").count()\n",
    "    print(f'Added {chunks_count} chunks to the knowledgebase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HalhxNB8-chE"
   },
   "outputs": [],
   "source": [
    "def do_rag(user_input: str, top_k: int = DEFAULT_TOP_K, verbose: bool = False) -> tuple[str, list[str], list[float], int]:\n",
    "  \"\"\" Runs the RAG pipeline with a given user input\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be used for the RAG pipeline\n",
    "  top_k : int, optional [default: DEFAULT_TOP_K]\n",
    "      The number of semantically closest chunks to be fetched\n",
    "  verbose : bool, optional [default: False]\n",
    "      Whether to print details of the RAG process or not. Defaults to False\n",
    "  Returns\n",
    "  -------\n",
    "  response: str\n",
    "      The generated response\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  distances: [float]\n",
    "      The corresponding distances to the user_input_embedding\n",
    "  \"\"\"\n",
    "  # Embed the user input\n",
    "  user_input_embedding = do_embed(chunk=user_input)\n",
    "\n",
    "  # \"R\" like \"Retrieval\": Get the k semantically closest chunks to the user input from the knowledgebase\n",
    "  retrieval_start_time = time.time()\n",
    "  (context, distances) = do_top_k_fetching(user_input_embedding=user_input_embedding, top_k=top_k)\n",
    "  if verbose:\n",
    "    retrieval_end_time = time.time()\n",
    "    retrieval_time = round(retrieval_end_time - retrieval_start_time, 2)\n",
    "\n",
    "  # \"A\" like \"Augmented\": Create the augmented prompt\n",
    "  augmented_prompt = augment(user_input=user_input, context=context)\n",
    "\n",
    "  # \"G\" like \"Generation\": Generate a response\n",
    "  generation_start_time = time.time()\n",
    "  (response, total_token_count) = generate_response(prompt=augmented_prompt)\n",
    "  if verbose:\n",
    "    generation_end_time = time.time()\n",
    "    generation_time = round(generation_end_time - generation_start_time, 2)\n",
    "    print(f'Retrieval took {retrieval_time}s. Generation took {generation_time}s.')\n",
    "\n",
    "  return (response, context, distances, total_token_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HS5VkCNHHCyb"
   },
   "outputs": [],
   "source": [
    "# Define a custom exception\n",
    "class FactCheckingValidationError(Exception):\n",
    "  \"\"\" Exception raised for errors in the fact checking validation. \"\"\"\n",
    "  pass\n",
    "\n",
    "# Define a response format\n",
    "class FactCheckingValidationAnswer(typing.TypedDict):\n",
    "  \"\"\" Response format for the fact checking validation. \"\"\"\n",
    "  is_grounded: bool\n",
    "\n",
    "\n",
    "def guard_fact_checking(bot_response: str, context: list[str]) -> str:\n",
    "  \"\"\" Uses fact checking validation for a given bot response and context\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  bot_response : str\n",
    "      The bot response to be guarded\n",
    "  context : [str]\n",
    "      The context to be used for the fact checking validation\n",
    "  Returns\n",
    "  -------\n",
    "  bot_response: str\n",
    "      The guarded bot response\n",
    "  \"\"\"\n",
    "  # Prepare the context to be used in the guard prompt\n",
    "  context = \"\\n\".join(context)\n",
    "\n",
    "  # Define the prompt for the guardrail\n",
    "  guard_prompt = f\"\"\"\n",
    "    You are given a task to identify if the answer is grounded and entailed to the context.\n",
    "    You will only use the contents of the context and not rely on external knowledge.\n",
    "    'context': {context} 'answer': {bot_response}\n",
    "    \"\"\"\n",
    "\n",
    "  # Call the guardrail model with the desired output format\n",
    "  model = genai.GenerativeModel(GUARDING_MODEL)\n",
    "  result = model.generate_content(\n",
    "      guard_prompt,\n",
    "      generation_config=genai.GenerationConfig(\n",
    "          response_mime_type=\"application/json\", response_schema=FactCheckingValidationAnswer\n",
    "      ),\n",
    "  )\n",
    "\n",
    "  # Evaluate the validation\n",
    "  fact_checking_validation = json.loads(result.text)\n",
    "  if not fact_checking_validation[\"is_grounded\"]:\n",
    "    error_msg = f\"The bot answer '{bot_response}' is not grounded in the context '{context}'\"\n",
    "    raise FactCheckingValidationError(error_msg)\n",
    "  return bot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80xBqIgCG7Lf"
   },
   "outputs": [],
   "source": [
    "def do_fact_checked_rag(user_input: str, top_k: int = DEFAULT_TOP_K, verbose: bool = False) -> tuple[str, list[str], list[float], int]:\n",
    "  \"\"\" Runs the RAG pipeline with a given user input and fact checking validation\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be used for the RAG pipeline\n",
    "  top_k : int, optional [default: DEFAULT_TOP_K]\n",
    "      The number of semantically closest chunks to be fetched\n",
    "  verbose : bool, optional [default: False]\n",
    "      Whether to print details of the RAG process or not. Defaults to False\n",
    "  Returns\n",
    "  -------\n",
    "  response: str\n",
    "      The generated response\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  distances: [float]\n",
    "      The corresponding distances to the user_input\n",
    "  total_token_count: int\n",
    "      The total token count for the generation\n",
    "  \"\"\"\n",
    "  (answer, context, distances, total_token_count) = do_rag(user_input=user_input, top_k=top_k, verbose=verbose)\n",
    "  try:\n",
    "    guarded_output = guard_fact_checking(bot_response=answer, context=context)\n",
    "  except FactCheckingValidationError:\n",
    "    # Return a predefined response if the fact checking validation fails\n",
    "    guarded_output = \"Possible hallucination detected.\"\n",
    "  return (guarded_output, context, distances, total_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xb_0IwRtCW6N"
   },
   "outputs": [],
   "source": [
    "def print_insights(dataframe: pd.DataFrame):\n",
    "  \"\"\" Prints the insights for a given evaluation dataframe\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  dataframe : pd.DataFrame\n",
    "      The dataframe to be analyzed\n",
    "  \"\"\"\n",
    "  median_response_time = round(dataframe['response_time'].median(), 2)\n",
    "  median_min_distance = round(dataframe['min_context_distance'].median(), 2)\n",
    "  median_median_distance = round(dataframe['median_context_distance'].median(), 2)\n",
    "  median_total_token_count = round(dataframe['total_token_count'].median(), 2)\n",
    "  n_hallucinations = dataframe[\"rag_response\"].value_counts().get(\"Possible hallucination detected.\", 0)\n",
    "\n",
    "  print(f'Number of detected hallucinations: {n_hallucinations}')\n",
    "  print(f'Median response time: {median_response_time} seconds')\n",
    "  print(f'Median of minimum distance: {median_min_distance}')\n",
    "  print(f'Median of median distance: {median_median_distance}')\n",
    "  print(f'Median total token count: {median_total_token_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwZ2yGQJ8dXg"
   },
   "outputs": [],
   "source": [
    "# The function gives us the desired outputs to gather some insights\n",
    "def generate_rag_answers(dataframe: pd.DataFrame, top_k: int = DEFAULT_TOP_K, verbose: bool = False):\n",
    "  \"\"\" Generates the RAG answers for a given dataframe\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  dataframe : pd.DataFrame\n",
    "      The dataframe, which includes the questions to be answered\n",
    "  top_k : int, optional [default: DEFAULT_TOP_K]\n",
    "      The number of semantically closest chunks to be fetched\n",
    "  verbose : bool, optional [default: False]\n",
    "      Whether to print details of the RAG process or not. Defaults to False\n",
    "  Returns\n",
    "  -------\n",
    "  dataframe: pd.DataFrame\n",
    "      The dataframe with the RAG answers\n",
    "  \"\"\"\n",
    "  def generate_rag_response_with_insights(question: str) -> pd.Series:\n",
    "        # Define start time for calculating the response time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Generate the RAG response\n",
    "        output = do_fact_checked_rag(question, top_k=top_k, verbose=verbose)\n",
    "        response = output[0]  # Extract the response from the output\n",
    "        distances = output[2]  # Extract the distances from the output\n",
    "        total_token_count = output[3]  # Extract the total token count from the output\n",
    "        min_distance = round(min(distances), 2)  # Find the minimum distance\n",
    "        median_distance = np.nanmedian(distances)  # Calculate the mean distance\n",
    "\n",
    "        # Define end time for calculating the response time\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate response time\n",
    "        response_time = round(end_time - start_time, 2)\n",
    "\n",
    "        # Return response and insights\n",
    "        return pd.Series([response, response_time, min_distance, median_distance, total_token_count])\n",
    "\n",
    "  # Apply the insights function to each row and store results in new columns\n",
    "  dataframe[['rag_response', 'response_time', 'min_context_distance', 'median_context_distance', 'total_token_count']] = dataframe['question'].apply(generate_rag_response_with_insights)\n",
    "  return dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqcAgsAzEl2A"
   },
   "source": [
    "### Configure the genAI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWcu0XZgEucE"
   },
   "outputs": [],
   "source": [
    "GENERATION_MODEL = \"gemini-1.5-flash\"\n",
    "EMBEDDING_MODEL = \"models/text-embedding-004\"\n",
    "GUARDING_MODEL = \"gemini-1.5-flash-8b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZQ7JcNG7PfJ"
   },
   "source": [
    "### Prepare the knowledgebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "LYbnjfkkj_Ni",
    "outputId": "40e43bfa-2175-4013-f0ef-5a7df0a4dd0c"
   },
   "outputs": [],
   "source": [
    "KNOWLEDGEBASE_CONTENT = set(['study_in_scarlett.txt'])\n",
    "\n",
    "do_ingestion(file_names=KNOWLEDGEBASE_CONTENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZshnk1L6KSX"
   },
   "source": [
    "### Manually evaluate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "jEfT2ks_6vCr",
    "outputId": "eb43c283-0508-4a96-9ac0-7b8dff8c962d"
   },
   "outputs": [],
   "source": [
    "# Read csv from local files\n",
    "evaluation_dataframe = pd.read_csv(EVALUATION_DATA_PATH + 'simple_evaluation_dataset.csv')\n",
    "evaluation_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "ZcvHFH1h81to",
    "outputId": "26b0afa5-9998-46b4-9aaf-497c48010e1b"
   },
   "outputs": [],
   "source": [
    "# Generate responses and insights. We use a wrapper function, which collects the insights and stores them in a dataframe\n",
    "evaluation_dataframe = generate_rag_answers(evaluation_dataframe)\n",
    "evaluation_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlam6qrjC3jF",
    "outputId": "ab3c143c-4daa-4537-cbbf-0bf0cf7d4681"
   },
   "outputs": [],
   "source": [
    "print_insights(evaluation_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSQUY8jiB7_m"
   },
   "source": [
    "### Ingest remaining stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "CTsNJS1ZCDC_",
    "outputId": "7e28362d-d834-4bbd-90c1-359ca863a668"
   },
   "outputs": [],
   "source": [
    "# Ingest the second book, which contains the other stories\n",
    "KNOWLEDGEBASE_CONTENT.add('adventures_of_sherlock_holmes.txt')\n",
    "do_ingestion(file_names=KNOWLEDGEBASE_CONTENT, clear_knowledgebase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "id": "G9TPRdheC7mM",
    "outputId": "2881552a-4600-4e3e-bf43-4f01fcd2016c"
   },
   "outputs": [],
   "source": [
    "# Generate responses and insights\n",
    "evaluation_dataframe = generate_rag_answers(evaluation_dataframe)\n",
    "evaluation_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jDndK5y7C6rO",
    "outputId": "848bd16e-8bbd-4ad9-8aeb-60846e67fead"
   },
   "outputs": [],
   "source": [
    "print_insights(evaluation_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zb2nSsO4dy5"
   },
   "source": [
    "### Manually adjust *chunk size*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XqyhcLW5BuA"
   },
   "outputs": [],
   "source": [
    "# Define new chunk size. Be careful: The smaller the chunk_size, the more time is needed for the ingestion. I.e. adjusted_chunk_size = 500 takes 1-2 minutes\n",
    "adjusted_chunk_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "kgbJJgwg4jbl",
    "outputId": "a16d33f4-f792-4211-fbb0-66fc46281818"
   },
   "outputs": [],
   "source": [
    "# We need to clear the knowledgebase for this\n",
    "do_ingestion(file_names=KNOWLEDGEBASE_CONTENT, chunk_size=adjusted_chunk_size, clear_knowledgebase=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "id": "qlhKcOnc5LPu",
    "outputId": "7d06c4f7-7336-4047-f09b-e4fc8b4de103"
   },
   "outputs": [],
   "source": [
    "# Generate responses and insights\n",
    "evaluation_dataframe = generate_rag_answers(evaluation_dataframe, verbose=True)\n",
    "evaluation_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qtwnl7jmC9qP",
    "outputId": "20672072-3770-4cd0-a269-2611e2f02992"
   },
   "outputs": [],
   "source": [
    "print_insights(evaluation_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9jVfnFw3dL3"
   },
   "source": [
    "### Manually adjust *top_k*\n",
    "We do not need to ingest again, since our adjustment just affects the retrieval pipeline. Keep in mind, that we use the chunking from before, because we did not update the knowledgebase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnvj7KQE3klw"
   },
   "outputs": [],
   "source": [
    "adjusted_top_k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "id": "ne0Z6BBz4QWp",
    "outputId": "5710a1d9-dcd8-4572-9f41-16764ff8d63d"
   },
   "outputs": [],
   "source": [
    "# Generate responses and insights\n",
    "evaluation_dataframe = generate_rag_answers(evaluation_dataframe, top_k=adjusted_top_k, verbose=True)\n",
    "evaluation_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3gpdkJsBC_Uw",
    "outputId": "b1236c1c-31e5-46f2-e350-166335c3171b"
   },
   "outputs": [],
   "source": [
    "print_insights(evaluation_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIt2bjPwA1ie"
   },
   "source": [
    "### Exercise 01: Find the best parameter values for *chunk_size* and *top_k*\n",
    "- **Main goal**: Generate a minimum amount of hallucinations\n",
    "- **Second goal**: Keep the total count of tokens as small as possible to reduce costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "_cuoL6JPB9WR",
    "outputId": "66e99e7f-db26-4f7b-bcf5-65fd4ea0e77c"
   },
   "outputs": [],
   "source": [
    "# We use a smaller set of evaluation question to reduce response time\n",
    "evaluation_dataframe = pd.read_csv(EVALUATION_DATA_PATH + 'simple_evaluation_dataset_shortened.csv')\n",
    "evaluation_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLF3oPNFBjkD"
   },
   "outputs": [],
   "source": [
    "# The knowledgebase consists of shortened versions of the books to reduce ingestion time. Do not change this.\n",
    "KNOWLEDGEBASE_CONTENT = ['study_in_scarlett_shortened.txt', 'adventures_of_sherlock_holmes_shortened.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QufW8mbxCQEK"
   },
   "outputs": [],
   "source": [
    "# TODO: Adjust the following parameters\n",
    "chunk_size = None\n",
    "top_k = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "Ho-OQMulC2Dz",
    "outputId": "46e22966-3a05-4098-912a-eed36cd3ec95"
   },
   "outputs": [],
   "source": [
    "# This needs to be executed if you changed chunk_size\n",
    "do_ingestion(file_names=KNOWLEDGEBASE_CONTENT, chunk_size=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "vEa4BrcIDDW8",
    "outputId": "beb702e5-be30-48bc-a8dd-ff65e3bc71ea"
   },
   "outputs": [],
   "source": [
    "# This needs to be executed always\n",
    "evaluation_dataframe = generate_rag_answers(evaluation_dataframe, top_k=top_k)\n",
    "evaluation_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TaPsIx3pDJpj",
    "outputId": "75f5cb36-e583-449e-f9fd-121fab7b9a70"
   },
   "outputs": [],
   "source": [
    "# TODO: Analyze the insights and iteratively find the best configuration for chunk_size and top_k\n",
    "print_insights(evaluation_dataframe)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "SxQu4eGGDrJe"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
