{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdNRFvKBjN3_"
   },
   "source": [
    "# GenAI Workshop\n",
    "## Lesson 5: RAG Evaluation & Optimization\n",
    "\n",
    "This lesson is intended to show you how different RAG configurations affect the output quality of a Retrieval Augmented Generation system.\n",
    "\n",
    "During this lesson you will learn how to ...\n",
    "\n",
    "- evaluate the quality of a RAG system using simple quality metrics\n",
    "- manipulate the *chunk size* and *top_k* to improve the system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxQu4eGGDrJe"
   },
   "source": [
    "### Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J595XWK4AuQ-",
    "outputId": "55b283aa-b58a-4781-c658-b39629096a89"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   COLAB = True\n",
    "   print(\"Running on COLAB environment.\")\n",
    "else:\n",
    "   COLAB = False\n",
    "   print(\"WARNING: Running on LOCAL environment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilgxHbeTwiJb"
   },
   "outputs": [],
   "source": [
    "# Clone the data repository into colab\n",
    "!git clone https://github.com/openknowledge/workshop-genai-data.git\n",
    "PROCESSED_DATA_PATH = \"/content/workshop-genai-data/processed/gutenberg/\"\n",
    "EVALUATION_DATA_PATH = \"/content/workshop-genai-data/evaluation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z06SMoItD5kL",
    "outputId": "48fa095d-0692-4e1b-e505-88aa7101efe5"
   },
   "outputs": [],
   "source": [
    "# Read and set GOOGLE_API_KEY depending of environment\n",
    "\n",
    "if COLAB:\n",
    "  # Note: GEMINI_API_KEY must be set as COLAB userdata before\n",
    "  print(\"Initialize COLAB environment\")\n",
    "  from google.colab import userdata\n",
    "  GOOGLE_API_KEY=userdata.get('GEMINI_API_KEY')\n",
    "else:\n",
    "  # GEMINI_API_KEY must be set as environment variable, e.g. inside an .env file.\n",
    "  print(\"Initialize LOCAL environment\")\n",
    "  from dotenv import load_dotenv, find_dotenv\n",
    "  load_dotenv(find_dotenv())\n",
    "  GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DqbDZeWD6nS"
   },
   "outputs": [],
   "source": [
    "# Initialize Google GenAI Client with GOOGLE_API_KEY to be able to call the model.\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFEsl___H2n4"
   },
   "outputs": [],
   "source": [
    "# Install additional libraries\n",
    "%%capture\n",
    "!pip install -qU langchain-text-splitters chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3UN1LX-IEcp"
   },
   "outputs": [],
   "source": [
    "# Import additional libraries\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from chromadb import EphemeralClient\n",
    "import uuid\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXY2UNBCBmDQ"
   },
   "outputs": [],
   "source": [
    "# Configure pandas display options\n",
    "pd.set_option(\"max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKG1EPEGdvvC"
   },
   "outputs": [],
   "source": [
    "# Set defaults for retrieval\n",
    "DEFAULT_TOP_K = 3\n",
    "DEFAULT_CHUNK_OVERLAP = 100\n",
    "DEFAULT_CHUNK_SIZE = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJW72wGgPOxt"
   },
   "outputs": [],
   "source": [
    "# This will be the chromadb collection we use as a knowledge base. We do not need the in-memory EphemeralClient.\n",
    "chromadb_client = EphemeralClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluation we need metadata. This function will additionally return the page number of the chunk\n",
    "def do_chunk(text: str, chunk_size: int , chunk_overlap: int) -> tuple[list[str], list[int]]:\n",
    "    \"\"\" Splits the text into chunks and maps them to page numbers\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to split\n",
    "    chunk_size : int\n",
    "        The size of the chunks\n",
    "    chunk_overlap : int\n",
    "        The overlap between the chunks\n",
    "        Returns\n",
    "    -------\n",
    "    tuple[list[str], list[int]]\n",
    "        Two lists: one with the chunks and another with the corresponding page numbers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define amount of characters per page. This is an approximation\n",
    "    CHARACTERS_PER_PAGE = 1800\n",
    "\n",
    "    # Initialize the splitter\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    # Split the text\n",
    "    chunks = splitter.split_text(text)\n",
    "\n",
    "    # Prepare the lists\n",
    "    chunk_texts = []\n",
    "    page_numbers = []\n",
    "\n",
    "    # Map chunks to page numbers\n",
    "    for chunk in chunks:\n",
    "        start_index = text.index(chunk)\n",
    "        page_number = (start_index // CHARACTERS_PER_PAGE) + 1\n",
    "        chunk_texts.append(chunk)\n",
    "        page_numbers.append(page_number)\n",
    "\n",
    "    return chunk_texts, page_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get the page number of a specific string in the book text\n",
    "def get_page_number(text: str, book_text: str, characters_per_page: int = 1800) -> int:\n",
    "    \"\"\"\n",
    "    Finds the page number of a specific string in the book text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The string to search for.\n",
    "    book_text : str\n",
    "        The complete book text.\n",
    "    characters_per_page : int, optional\n",
    "        The approximate number of characters per page, by default 1800.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The page number where the string is found, or -1 if not found.\n",
    "    \"\"\"\n",
    "    # Find the starting index of the string\n",
    "    start_index = book_text.find(text)\n",
    "\n",
    "    # If the string is not found, return -1\n",
    "    if start_index == -1:\n",
    "        return -1\n",
    "\n",
    "    # Calculate the page number\n",
    "    page_number = (start_index // characters_per_page) + 1\n",
    "    return page_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since now we store metadata in the knowledgebase, we need to update the fetching function in order to\n",
    "# return the metadata as well. We will use the pydantic library to define the chunk model.\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    \"\"\"Represents the metadata of a document which is stored in the knowledgebase.\"\"\"\n",
    "    page: int\n",
    "    book: str\n",
    "\n",
    "class FetchedChunk(BaseModel):\n",
    "    \"\"\"Represents a chunk fetched from the knowledgebase.\"\"\"\n",
    "    chunk: str\n",
    "    metadata: Metadata\n",
    "\n",
    "# Building Block \"Top-k Fetching\": Get the k semantically closest chunks to the user input from the knowledgebase\n",
    "def do_top_k_fetching(user_input_embedding: list[float], top_k: int) -> list[FetchedChunk]:\n",
    "  \"\"\" Fetches the k semantically closest chunks to the user input from the knowledgebase\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input_embedding : [float]\n",
    "      The embedding of the user input\n",
    "  top_k : int\n",
    "      The number of semantically closest chunks to be fetched\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  \"\"\"\n",
    "  # Since we will do the fetching always only for one user_input,\n",
    "  # instead of querying for multiple embeddings simultanously as allowed by the choma API,\n",
    "  # we add the embeddings below to a list and return only the first document (chunk)\n",
    "  chromadb_collection = chromadb_client.get_collection(name=\"default\")\n",
    "\n",
    "  query_result = chromadb_collection.query(\n",
    "      query_embeddings=[user_input_embedding],\n",
    "      n_results=top_k,\n",
    "  )\n",
    "  chunks = query_result[\"documents\"][0]\n",
    "  metadatas = query_result[\"metadatas\"][0]\n",
    "  \n",
    "  fetched_chunks = []\n",
    "  for i in range(len(chunks)):\n",
    "    chunk = chunks[i]\n",
    "    metadata = metadatas[i]\n",
    "    fetched_chunk = FetchedChunk(chunk=chunk, metadata=Metadata(**metadata))\n",
    "    fetched_chunks.append(fetched_chunk)\n",
    "  return fetched_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bqI0tu1xfKaR"
   },
   "outputs": [],
   "source": [
    "# RAG building blocks\n",
    "\n",
    "# Get content of books. The content will already be cleansed.\n",
    "def load_file_content(file_name: str) -> str:\n",
    "  \"\"\" Loads content of a file in the local file systemby a given file name and return its content\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        The name of the file to be loaded\n",
    "    Returns\n",
    "    -------\n",
    "    file_content : str\n",
    "        the content of the file as a string\n",
    "    \"\"\"\n",
    "  with open(f\"{PROCESSED_DATA_PATH}{file_name}\", \"r\") as f:\n",
    "    return f.read()\n",
    "\n",
    "# Building Block \"Embedding\": Create multi dimensional embeddings for a given chunk.\n",
    "def do_embed(chunk: str) -> list[float]:\n",
    "  \"\"\" Embeds a given chunk and returns the embedding\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  chunk : str\n",
    "      The chunk to be embedded\n",
    "  Returns\n",
    "  -------\n",
    "  embedding: [float]\n",
    "      The created embedding\n",
    "  \"\"\"\n",
    "  return genai.embed_content(model=EMBEDDING_MODEL, content=chunk).get(\"embedding\")\n",
    "\n",
    "def do_batch_embed(chunks: list[str]) -> list[list[float]]:\n",
    "  \"\"\" Embeds a list of chunks and returns the embeddings\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  chunks : [str]\n",
    "      The chunks to be embedded\n",
    "  Returns\n",
    "  -------\n",
    "  embeddings: [list[float]]\n",
    "      The created embeddings\n",
    "  \"\"\"\n",
    "  return genai.embed_content(model=EMBEDDING_MODEL, content=chunks).get(\"embedding\")\n",
    "\n",
    "# Building Block \"Knowledgebase\": Store embeddings and the corresponding content in a vectorstore\n",
    "def persist_embeddings(chunks: list[str], embeddings: list[float], metadatas: list[dict])-> None:\n",
    "  \"\"\" Persists the embeddings and the chunks in the knowledgebase\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  chunks : [str]\n",
    "      The chunks to be persisted\n",
    "  embeddings: [list[float]]\n",
    "      The corresponding embeddings to be persisted\n",
    "  metadatas: list[dict]\n",
    "      The corresponding metadata to be persisted\n",
    "\n",
    "  \"\"\"\n",
    "  # We need to define hhnsw:search_ef for determinstic results (see https://github.com/chroma-core/chroma/issues/2675)\n",
    "  chromadb_collection = chromadb_client.get_or_create_collection(name=\"default\", metadata={\"hnsw:search_ef\":50})\n",
    "  # Persist the embeddings and the chunks in the knowledgebase\n",
    "  ids = [str(uuid.uuid4()) for _ in chunks]\n",
    "  chromadb_collection.add(ids=ids, documents=chunks, embeddings=embeddings, metadatas=metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2W1KNEU7fKaR"
   },
   "outputs": [],
   "source": [
    "# We need to update the ingestion in order to store the page number of the chunk\n",
    "def do_ingestion(file_names: list[str], chunk_size: int = DEFAULT_CHUNK_SIZE, clear_knowledgebase: bool = False, verbose: bool = False) -> None:\n",
    "  \"\"\" Ingests a list of files by a given file name and chunk\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  file_names : [str]\n",
    "      The names of the files to be ingested\n",
    "  chunk_size : int, optional [default: DEFAULT_CHUNK_SIZE]\n",
    "      The desired chunk size\n",
    "  clear_knowledgebase : bool, optional [default: False]\n",
    "      Whether to clear the knowledgebase before ingesting the new files\n",
    "  verbose : bool, optional [default: False]\n",
    "      Whether to print details of the ingestion process or not. Defaults to False\n",
    "  \"\"\"\n",
    "\n",
    "  # Check chunk_size in order to avoid running too long\n",
    "  if chunk_size < 250:\n",
    "    raise ValueError(\"chunk_size must higher than 250\")\n",
    "  \n",
    "  if clear_knowledgebase:\n",
    "    chromadb_client.delete_collection(name=\"default\")\n",
    "\n",
    "  # Ingest file by file\n",
    "  for file_name in file_names:\n",
    "    # Load prepared book content\n",
    "    file_content = load_file_content(file_name)\n",
    "\n",
    "    # Chunk the content into smaller chunks\n",
    "    chunks, page_numbers = do_chunk(file_content, chunk_size=chunk_size, chunk_overlap=DEFAULT_CHUNK_OVERLAP)\n",
    "    if verbose:\n",
    "      print(f'Loaded {len(chunks)} chunks from {file_name}')\n",
    "\n",
    "    # Embed the chunks\n",
    "    embeddings = do_batch_embed(chunks)\n",
    "\n",
    "    # Metadata: Create a list of dictionaries with the page number\n",
    "    metadatas = [{\"page\": page, \"book\": file_name} for page in page_numbers]\n",
    "\n",
    "    # Persist the embeddings and the chunks in the knowledgebase\n",
    "    persist_embeddings(chunks, embeddings, metadatas=metadatas)\n",
    "\n",
    "  if verbose:\n",
    "    chunks_count = chromadb_client.get_collection(name=\"default\").count()\n",
    "    print(f'Added {chunks_count} chunks to the knowledgebase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will only evaluate the retrieval, since this is the most important part of the pipeline\n",
    "# In a real world scenario, we would also evaluate the generation using i.e. llm-based metrics\n",
    "def do_retrieval(user_input: str, top_k: int = DEFAULT_TOP_K) -> list[FetchedChunk]:\n",
    "  \"\"\" Retrieves the k semantically closest chunks to the user input from the knowledgebase\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be used for the retrieval\n",
    "  top_k : int, optional [default: DEFAULT_TOP_K]\n",
    "      The number of semantically closest chunks to be fetched\n",
    "  Returns\n",
    "  -------\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  distances: [float]\n",
    "      The corresponding distances to the user_input_embedding\n",
    "  \"\"\"\n",
    "  # Embed the user input\n",
    "  user_input_embedding = do_embed(user_input)\n",
    "\n",
    "  # Fetch the k semantically closest chunks to the user input from the knowledgebase\n",
    "  return do_top_k_fetching(user_input_embedding, top_k=top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqcAgsAzEl2A"
   },
   "source": [
    "### Configure the genAI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWcu0XZgEucE"
   },
   "outputs": [],
   "source": [
    "# We will only evaluate the retrieval, therefore we only need an embedding model\n",
    "EMBEDDING_MODEL = \"models/text-embedding-004\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZQ7JcNG7PfJ"
   },
   "source": [
    "### Prepare the knowledgebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYbnjfkkj_Ni"
   },
   "outputs": [],
   "source": [
    "KNOWLEDGEBASE_CONTENT = set(['study_in_scarlett.txt'])\n",
    "\n",
    "do_ingestion(file_names=KNOWLEDGEBASE_CONTENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingested Metadata\n",
    "For this exercise, we enhanced our knowledgebase with metadata. This metadata contains information about the chunk: the file name of the book and page number, where the chunk is from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a close look on the metadata\n",
    "fetched_chunks = do_retrieval(user_input=\"Which number did Lucie noticed on the ceiling?\")\n",
    "\n",
    "print(f\"Fetched {len(fetched_chunks)} chunks from the knowledgebase\")\n",
    "for chunk in fetched_chunks:\n",
    "    print(f\"Chunk: {chunk.chunk}\")\n",
    "    print(f\"Metadata: {chunk.metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to identify the origin of a chunk, we use the following concatenation of the book name and the page number\n",
    "def get_origin(chunk: FetchedChunk) -> str:\n",
    "    \"\"\" Returns the origin of a chunk by concatenating the book name and the page number\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk : FetchedChunk\n",
    "        The chunk to be used for the origin\n",
    "    Returns\n",
    "    -------\n",
    "    origin: str\n",
    "        The origin of the chunk\n",
    "    \"\"\"\n",
    "    return f\"{chunk.metadata.book}:{chunk.metadata.page}\"\n",
    "\n",
    "# This is the origin of a chunk\n",
    "print(f\"Metadata of the chunk: \\t{fetched_chunks[0].metadata}\")\n",
    "print(f\"Origin of the chunk: \\t{get_origin(fetched_chunks[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "Matching the *text of the retrieved chunk* with *the text of the ground truth chunk* introduces some challenges caused by differing chunk sizes between retrieved and ground truth chunks. Having metadata is perfect for information retrieval metrics, because we can match the *retrieved metadata* with the *ground truth metadata*. For this reason, we will use the following three metrics for matching metadata:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hit Rate\n",
    "Hit Rate is an information retrieval metric that measures whether at least one relevant result (*ground truth chunk*) appears in a list of search results (*retrieved chunks*). If there is at least one relevant item, the hit rate is 1; if there are none, it is 0. It simply checks for the presence of any relevant result, regardless of its position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the hit rate\n",
    "def calculate_hit_rate(ground_truth, retrieved: list[str]) -> int:\n",
    "    gt_set = set(ground_truth)\n",
    "    retrieved_set = set(retrieved)\n",
    "    return int(bool(gt_set & retrieved_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets test Hit Rate. Try it yourself with different values\n",
    "ground_truth = [\"study_in_scarlett.txt:22\"]\n",
    "retrieved = [\"study_in_scarlett.txt:22\", \"study_in_scarlett.txt:23\"]\n",
    "print(f\"Hit Rate: {calculate_hit_rate(ground_truth, retrieved)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reciprocal Rank\n",
    "Reciprocal Rank is an information retrieval metric that measures how soon the first relevant result (*ground truth chunk*) appears in a list of search results (*retrieved chunks*). It is calculated as the inverse (1 divided by the rank) of the position of the first relevant result. For example, if the first relevant result is at position 3, the reciprocal rank is 1/3. If the retrieved chunks do not contain any ground truth chunk, the reciprocal rank is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the calculate_reciprocal_rank\n",
    "def calculate_reciprocal_rank(ground_truth: list[str], retrieved: list[str]) -> float:\n",
    "    gt_set = set(ground_truth)\n",
    "    for rank, item in enumerate(retrieved, start=1):\n",
    "        if item in gt_set:\n",
    "            return 1 / rank\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test Reciprocal Rank. Try it yourself with different values\n",
    "ground_truth = [\"study_in_scarlett.txt:22\"]\n",
    "retrieved = [\"adventures_of_sherlock_holmes.txt:903\",\"study_in_scarlett.txt:22\", \"study_in_scarlett.txt:3\"]\n",
    "print(f\"Reciprocal Rank: {calculate_reciprocal_rank(ground_truth, retrieved)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "Precision is an information retrieval metric that measures the proportion of retrieved results that are actually relevant. It is calculated as the number of relevant results divided by the total number of results returned (*top-k*). For example, if 1 out of 4 retrieved documents are relevant (present in the *ground truth*), the precision is 0.25 (25%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the precision\n",
    "def calculate_precision(ground_truth: list[str], retrieved: list[str]) -> float:\n",
    "    # Number of retrieved items\n",
    "    top_k = len(retrieved)\n",
    "\n",
    "    # Elements, which are present in both ground truth and retrieved\n",
    "    relevant_items_in_retrieved = (set(ground_truth) & set(retrieved))\n",
    "\n",
    "    return len(relevant_items_in_retrieved) / top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test Precision. Try it yourself with different values\n",
    "ground_truth = [\"study_in_scarlett.txt:22\"]\n",
    "retrieved = [\"adventures_of_sherlock_holmes.txt:903\",\"study_in_scarlett.txt:22\", \"study_in_scarlett.txt:3\",\"adventures_of_sherlock_holmes.txt:10\"]\n",
    "print(f\"Precision: {calculate_precision(ground_truth, retrieved)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Measure the current performance of the retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testdata\n",
    "evaluation_dataframe = pd.read_csv(EVALUATION_DATA_PATH + 'evaluation_dataset.csv')\n",
    "evaluation_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run evaluation fo specific top_k\n",
    "def evaluate_retrieval(top_k: int = DEFAULT_TOP_K) -> tuple[float, float, float]:\n",
    "    \"\"\" Evaluates the retrieval performance using precision, reciprocal rank and hit rate\n",
    "    Parameters\n",
    "    ----------\n",
    "    top_k : int, optional [default: DEFAULT_TOP_K]\n",
    "        The number of semantically closest chunks to be fetched\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[float, float, float]\n",
    "        The precision, reciprocal rank and hit rate\n",
    "    \"\"\"\n",
    "    # Create empty lists to store the metrics\n",
    "    precisions = []\n",
    "    reciprocal_ranks = []\n",
    "    hit_rates = []\n",
    "\n",
    "    for _, row in evaluation_dataframe.iterrows():\n",
    "        # Get the question and the ground truth origin\n",
    "        question = row['question']\n",
    "        ground_truth_origin = row['ground_truth_origin']\n",
    "\n",
    "        # Retrieve the chunks\n",
    "        retrieved_chunks = do_retrieval(question, top_k=top_k)\n",
    "\n",
    "        # Get the origins of the retrieved chunks\n",
    "        retrieved_origins = [get_origin(chunk) for chunk in retrieved_chunks]\n",
    "\n",
    "        # Calculate the metrics\n",
    "        precisions.append(calculate_precision([ground_truth_origin], retrieved_origins))\n",
    "        reciprocal_ranks.append(calculate_reciprocal_rank([ground_truth_origin], retrieved_origins))\n",
    "        hit_rates.append(calculate_hit_rate([ground_truth_origin], retrieved_origins))\n",
    "    # Calculate the average metrics\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_reciprocal_rank = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "    avg_hit_rate = sum(hit_rates) / len(hit_rates)\n",
    "    return (avg_precision, avg_reciprocal_rank, avg_hit_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the performance of the retrieval\n",
    "\n",
    "# Configure retrieval\n",
    "top_k = 3\n",
    "\n",
    "# Evaluate the retrieval performance\n",
    "mean_precision, mean_reciprocal_rank, mean_hit_rate = evaluate_retrieval(top_k=top_k)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average Precision: {mean_precision:.2f}\")\n",
    "print(f\"Average Reciprocal Rank: {mean_reciprocal_rank:.2f}\")\n",
    "print(f\"Average Hit Rate: {mean_hit_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSQUY8jiB7_m"
   },
   "source": [
    "### Ingest remaining stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTsNJS1ZCDC_"
   },
   "outputs": [],
   "source": [
    "# Ingest the second book, which contains the other stories\n",
    "KNOWLEDGEBASE_CONTENT.add('adventures_of_sherlock_holmes.txt')\n",
    "do_ingestion(file_names=KNOWLEDGEBASE_CONTENT, clear_knowledgebase=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 01: Evaluate RAG (again)\n",
    "The initial performance of the retrieval was quite low. A possible explanation might be, that we asked the system about a book, we did not ingest. Now, that we ingested the remaining book, it is your task to measure the performance again.  \n",
    "**Hint**: If you need to cancel the process, make sure that you restart the runtime and execute alle code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "id": "G9TPRdheC7mM",
    "outputId": "2881552a-4600-4e3e-bf43-4f01fcd2016c"
   },
   "outputs": [],
   "source": [
    "# TODO: Measure the performance of the retrieval with the second book\n",
    "\n",
    "# TODO: Evaluate the retrieval performance\n",
    "mean_precision, mean_reciprocal_rank, mean_hit_rate = None\n",
    "\n",
    "# TODO: Print the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 02: Optimize RAG\n",
    "Now that we know, how the retrieval performs, we should improve it. It is your task, to improve the performance by manipulating two variables: *top-k* and *chunk-size*. Keep in mind that you have to run the ingestion pipeline again, as soon as you change the *chunk size*. Try to find reasons, why the metrics increase/decrease the way they do!  \n",
    "**Hint**: If you need to cancel the process, make sure that you restart the runtime and execute alle code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the optimal configuration\n",
    "# Be careful: Do not set the chunk_size below 250, otherwise the ingestion will take too long\n",
    "chunk_size = None   # TODO: Set the chunk size\n",
    "top_k = None        # TODO: Set the top k\n",
    "\n",
    "# Ingest both books\n",
    "do_ingestion(file_names=KNOWLEDGEBASE_CONTENT, chunk_size=chunk_size, clear_knowledgebase=True)\n",
    "\n",
    "# Evaluate the retrieval performance\n",
    "mean_precision, mean_reciprocal_rank, mean_hit_rate = evaluate_retrieval(top_k=top_k)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average Precision: {mean_precision:.2f}\")\n",
    "print(f\"Average Reciprocal Rank: {mean_reciprocal_rank:.2f}\")\n",
    "print(f\"Average Hit Rate: {mean_hit_rate:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "SxQu4eGGDrJe"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
