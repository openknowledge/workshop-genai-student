{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GenAI Workshop\n",
    "## Lesson 3: Basic RAG\n",
    "\n",
    "This lesson is intended to show you the basics of a Retrieval Augmented Generation (RAG) system.\n",
    "\n",
    "During this lesson you will learn how to ...\n",
    "\n",
    "- implement the different building blocks of RAG\n",
    "- create a ingestion pipeline from the building blocks\n",
    "- create a retrieval pipeline from the building blocks\n",
    "- use a RAG system to generate responses to user inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxQu4eGGDrJe"
   },
   "source": [
    "### Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J595XWK4AuQ-",
    "outputId": "1e62adfb-80c3-4c90-cc56-de64df140dbd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   COLAB = True\n",
    "   print(\"Running on COLAB environment.\")\n",
    "else:\n",
    "   COLAB = False\n",
    "   print(\"WARNING: Running on LOCAL environment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the data repository into colab\n",
    "!git clone https://github.com/openknowledge/workshop-genai-data.git\n",
    "PROCESSED_DATA_PATH = \"/content/workshop-genai-data/processed/gutenberg/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "z06SMoItD5kL"
   },
   "outputs": [],
   "source": [
    "# import colab specific lib to read user data (aka colab managed secrets)\n",
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "-DqbDZeWD6nS"
   },
   "outputs": [],
   "source": [
    "# Initialize Google GenAI Client API with GOOGLE_API_KEY to be able to call the model.\n",
    "# Note: GEMINI_API_KEY must be set as COLAB userdata before!\n",
    "GOOGLE_API_KEY=userdata.get('GEMINI_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "OFEsl___H2n4"
   },
   "outputs": [],
   "source": [
    "# Install additional libraries\n",
    "%%capture\n",
    "!pip install -qU langchain-text-splitters\n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3UN1LX-IEcp"
   },
   "outputs": [],
   "source": [
    "# Import additional libraries\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from chromadb import EphemeralClient\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKG1EPEGdvvC"
   },
   "outputs": [],
   "source": [
    "# Set default values for model, model parameters and prompt\n",
    "DEFAULT_MODEL = \"gemini-1.5-flash\"\n",
    "DEFAULT_CONFIG_TEMPERATURE = 0.9\n",
    "DEFAULT_CONFIG_TOP_K = 1\n",
    "DEFAULT_CONFIG_MAX_OUTPUT_TOKENS = 200\n",
    "DEFAULT_SYSTEM_PROMPT = \"Your are a friendly assistant\"\n",
    "DEFAULT_USER_PROMPT = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJW72wGgPOxt"
   },
   "outputs": [],
   "source": [
    "# This will be the chromadb collection we use as a knowledge base. We do not need the in-memory EphemeralClient.\n",
    "chromadb_collection = EphemeralClient().get_or_create_collection(name=\"default\")\n",
    "\n",
    "# Have a look into the knowledgebase\n",
    "def peek_knowledgebase():\n",
    "  \"\"\"Shows the first ten items of the knowledgebase\"\"\"\n",
    "  print(chromadb_collection.peek())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4s8AAOATdqCa"
   },
   "outputs": [],
   "source": [
    "def call_genai_model_for_completion(\n",
    "        model_name: str = DEFAULT_MODEL,\n",
    "        config_temperature:float = DEFAULT_CONFIG_TEMPERATURE,\n",
    "        config_top_k: int = DEFAULT_CONFIG_TOP_K,\n",
    "        config_max_output_tokens: int = DEFAULT_CONFIG_MAX_OUTPUT_TOKENS,\n",
    "        system_prompt : str = DEFAULT_SYSTEM_PROMPT,\n",
    "        user_prompt : str = DEFAULT_USER_PROMPT,\n",
    "        verbose: bool = False\n",
    "        ):\n",
    "    \"\"\" Calls a gemini model with a given set of parameters and returns the completions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str, optional [default: DEFAULT_MODEL]\n",
    "        The name of the model to use for the completion\n",
    "    temperature : float, optional [default: DEFAULT_CONFIG_TEMPERATURE]\n",
    "        The temperature of the model\n",
    "    top_k : int, optional [default: DEFAULT_CONFIG_TOP_K]\n",
    "        The number of most recent matches to return\n",
    "    max_output_tokens : int, optional [default: DEFAULT_CONFIG_MAX_OUTPUT_TOKENS]\n",
    "        The maximum number of output tokens to return\n",
    "    system_prompt : str, optional [default: DEFAULT_SYSTEM_PROMPT]\n",
    "        The system prompt to use for the completion\n",
    "    user_prompt : str, optional [default: DEFAULT_USER_PROMPT]\n",
    "        The user prompt to use for the completion\n",
    "    file_list : [str], optional [default: empty list]\n",
    "    verbose : bool, optional [default: False]\n",
    "        Whether to print details of the completion process or not. Defaults to False\n",
    "    Returns\n",
    "    -------\n",
    "    completions :\n",
    "        a GenerateContentResponse instance representing the genAI model answer(s)\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        # print out summary of input values / parameters\n",
    "        print(f'Generating answer for following config:')\n",
    "        print(f'  - SYSTEM PROMPT used:\\n {system_prompt}')\n",
    "        print(f'  - USER PROMPT used:\\n {user_prompt}')\n",
    "        print(f'  - MODEL used:\\n {model_name} (temperature = {config_temperature}, top_k = {config_top_k}, max_output_tokens = {config_max_output_tokens})')\n",
    "\n",
    "    # create generation config\n",
    "    model_config = genai.GenerationConfig(\n",
    "        max_output_tokens=config_max_output_tokens,\n",
    "        temperature=config_temperature,\n",
    "        top_k=config_top_k\n",
    "    )\n",
    "\n",
    "    # create genai model with generation config\n",
    "    genai_model = genai.GenerativeModel(\n",
    "        model_name= model_name,\n",
    "        generation_config= model_config\n",
    "    )\n",
    "\n",
    "    response = genai_model.generate_content([system_prompt, user_prompt])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4fP0xbJeLAK"
   },
   "outputs": [],
   "source": [
    "def print_completion_result(completion_result, full:bool = False):\n",
    "    \"\"\" Prints the completion result of a genAI model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    completion_result :\n",
    "        a GenerateContentResponse instance representing the genAI model answer(s)\n",
    "    full : bool, optional [default: False]\n",
    "        Whether to print the full completion result or not. Defaults to False\n",
    "    \"\"\"\n",
    "    # print out answer of genai model (aka text of response)\n",
    "    print(f'\\nANSWER of genAI model: \\n')\n",
    "    if full:\n",
    "        print(completion_result)\n",
    "    else:\n",
    "        print(completion_result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqcAgsAzEl2A"
   },
   "source": [
    "### Configure the genAI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "zWcu0XZgEucE"
   },
   "outputs": [],
   "source": [
    "GENERATION_MODEL = \"gemini-1.5-flash\"\n",
    "EMBEDDING_MODEL = \"models/text-embedding-004\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EigTzQ_eAWd1"
   },
   "source": [
    "### Configure retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "FeNQxSjSALrG"
   },
   "outputs": [],
   "source": [
    "DEFAULT_K = 3\n",
    "DEFAULT_CHUNK_SIZE = 2000\n",
    "DEFAULT_CHUNK_OVERLAP = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xd-X4JzHCBui"
   },
   "source": [
    "### Define RAG Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sd_hrTtE-chD"
   },
   "outputs": [],
   "source": [
    "# Get content of books. The content will already be cleansed.\n",
    "def load_file_content(file_name: str) -> str:\n",
    "  \"\"\" Loads content of a file in the local file systemby a given file name and return its content\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        The name of the file to be loaded\n",
    "    Returns\n",
    "    -------\n",
    "    file_content : str\n",
    "        the content of the file as a string\n",
    "  \"\"\"\n",
    "  with open(f\"{PROCESSED_DATA_PATH}{file_name}\", \"r\") as f:\n",
    "    return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BquBXbJxBYWd"
   },
   "outputs": [],
   "source": [
    "# Building Block \"Chunking\": Split the content into smaller chunks\n",
    "def do_chunk(text: str) -> list[str]:\n",
    "  \"\"\" Chunks a given text by a given chunk size and chunk overlap and returns a list of chunks\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "      The text to be chunked\n",
    "  chunk_size : int, optional [default: DEFAULT_CHUNK_SIZE]\n",
    "        The desired chunk size\n",
    "  chunk_overlap : int, optional [default: DEFAULT_CHUNK_OVERLAP]\n",
    "        The desired chunk overlap\n",
    "  Returns\n",
    "  -------\n",
    "  chunks: [str]\n",
    "      The created chunks\n",
    "  \"\"\"\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "      chunk_size=DEFAULT_CHUNK_SIZE,\n",
    "      chunk_overlap=DEFAULT_CHUNK_OVERLAP,\n",
    "      length_function=len,\n",
    "  )\n",
    "  return text_splitter.split_text(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cuFXzCyBxdU"
   },
   "outputs": [],
   "source": [
    "# Building Block \"Embedding\": Create multi dimensional embeddings for a given chunk.\n",
    "def do_embed(chunk: str) -> list[float]:\n",
    "  \"\"\" Embeds a given chunk and returns the embedding\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  chunk : str\n",
    "      The chunk to be embedded\n",
    "  Returns\n",
    "  -------\n",
    "  embedding: [float]\n",
    "      The created embedding\n",
    "  \"\"\"\n",
    "  return genai.embed_content(model=EMBEDDING_MODEL, content=chunk).get(\"embedding\")\n",
    "\n",
    "def do_batch_embed(chunks: list[str]) -> list[list[float]]:\n",
    "  \"\"\" Embeds a list of chunks and returns the embeddings\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  chunks : [str]\n",
    "      The chunks to be embedded\n",
    "  Returns\n",
    "  -------\n",
    "  embeddings: [list[float]]\n",
    "      The created embeddings\n",
    "  \"\"\"\n",
    "  return genai.embed_content(model=EMBEDDING_MODEL, content=chunks).get(\"embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpJduX2pSzxi"
   },
   "outputs": [],
   "source": [
    "# Building Block \"Knowledgebase\": Store embeddings and the corresponding content in a vectorstore\n",
    "def persist_embeddings(chunks: list[str], embeddings: list[float])-> None:\n",
    "  \"\"\" Persists the embeddings and the chunks in the knowledgebase\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  chunks : [str]\n",
    "      The chunks to be persisted\n",
    "  embeddings: [list[float]]\n",
    "      The corresponding embeddings to be persisted\n",
    "  \"\"\"\n",
    "  ids = [str(uuid.uuid4()) for _ in chunks]\n",
    "  chromadb_collection.add(ids=ids, documents=chunks, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fyut_CCPC37U"
   },
   "outputs": [],
   "source": [
    "# Building Block \"Augmentation\": Create an updated prompt by merging the original user input with the provided context\n",
    "def augment(user_input: str, context: list[str]) -> str:\n",
    "  \"\"\" Augments a given user input by merging it with the provided context and returns the augmented prompt\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be augmented\n",
    "  context : [str]\n",
    "      The context to be merged with the user input\n",
    "  Returns\n",
    "  -------\n",
    "  augmented_prompt: str\n",
    "      The created augmented prompt\n",
    "  \"\"\"\n",
    "  prepared_context = \"\\n\".join(context)\n",
    "  augmented_prompt = f\"\"\"\n",
    "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "    provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "    Context:\\n{prepared_context}?\\n\n",
    "    Question: \\n{user_input}\\n\n",
    "\n",
    "    Answer:\n",
    "  \"\"\"\n",
    "  return augmented_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKHUg84XDLRp"
   },
   "outputs": [],
   "source": [
    "# Building Block \"Top-k Fetching\": Get the k semantically closest chunks to the user input from the knowledgebase\n",
    "def do_top_k_fetching(user_input_embedding: list[float], top_k: int) -> list[str]:\n",
    "  \"\"\" Fetches the k semantically closest chunks to the user input from the knowledgebase\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input_embedding : [float]\n",
    "      The embedding of the user input\n",
    "  top_k : int\n",
    "      The number of semantically closest chunks to be fetched\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  \"\"\"\n",
    "  # Since we will do the fetching always only for one user_input,\n",
    "  # instead of querying for multiple embeddings simultanously as allowed by the choma API,\n",
    "  # we add the embeddings below to a list and return only the first document (chunk)\n",
    "  return chromadb_collection.query(\n",
    "      query_embeddings=[user_input_embedding],\n",
    "      n_results=top_k,\n",
    "  )[\"documents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARQ8Pxo1d7kq"
   },
   "outputs": [],
   "source": [
    "# Building Block \"Generation\": Use the generation model to create a response\n",
    "def generate_response(prompt: str) -> None:\n",
    "  \"\"\" Generates a response for a given prompt\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  prompt : str\n",
    "      The prompt to be used for the generation\n",
    "  \"\"\"\n",
    "  completion_result = call_genai_model_for_completion(\n",
    "      model_name=GENERATION_MODEL,\n",
    "      user_prompt=prompt,\n",
    "  )\n",
    "  print_completion_result(completion_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7A5e_dY-chC"
   },
   "source": [
    "### Create the ingestion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HalhxNB8-chE"
   },
   "outputs": [],
   "source": [
    "def do_ingestion(file_names: list[str]) -> None:\n",
    "  \"\"\" Ingests a list of files by a given file name\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  file_names : [str]\n",
    "      The names of the files to be ingested\n",
    "  \"\"\"\n",
    "  # Ingest file by file\n",
    "  for file_name in file_names:\n",
    "    # Load prepared book content\n",
    "    file_content = load_file_content(file_name)\n",
    "\n",
    "    # Chunk the content into smaller chunks\n",
    "    chunks = do_chunk(file_content)\n",
    "\n",
    "    # Embed the chunks\n",
    "    embeddings = do_batch_embed(chunks)\n",
    "\n",
    "    # Persist the embeddings and the chunks in the knowledgebase\n",
    "    persist_embeddings(chunks, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8u56sn0894D4"
   },
   "source": [
    "### Perform ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "GC2yAP9QWN2x"
   },
   "outputs": [],
   "source": [
    "# Define file names to be ingested\n",
    "file_names = ['study_in_scarlett.txt']\n",
    "\n",
    "# Perform ingestion. Depending on the chunk_size this might take some minutes.\n",
    "do_ingestion(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DaQ8u7tj_UVG",
    "outputId": "246c7f2a-122f-40c3-cf2b-38af7dff8f7e"
   },
   "outputs": [],
   "source": [
    "# Use helper function to peek into knowledgebase\n",
    "peek_knowledgebase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAKlDC8WclSD"
   },
   "source": [
    "### Exercise 01: Create RAG pipeline\n",
    "In this exercise you will create a rag pipeline for retrieving relevant chunks and generating a grounded response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tehtlj-T_wDc"
   },
   "outputs": [],
   "source": [
    "def do_rag(user_input: str, verbose: bool = False) -> None:\n",
    "  \"\"\" Runs the RAG pipeline with a given user input and prints the response\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be used for the RAG pipeline\n",
    "  verbose : bool, optional [default: False]\n",
    "      Whether to print details of the RAG process or not. Defaults to False\n",
    "  \"\"\"\n",
    "    \n",
    "  # TODO: Embed the user input\n",
    "  user_input_embedding = None\n",
    "\n",
    "  # TODO: \"R\" like \"Retrieval\": Get the k semantically closest chunks to the user input from the knowledgebase\n",
    "  context = None\n",
    "  if verbose:\n",
    "    print(f'Retrieved context:\\n {context}')\n",
    "\n",
    "  # TODO: \"A\" like \"Augmented\": Create the augmented prompt\n",
    "  augmented_prompt = None\n",
    "  if verbose:\n",
    "    print(f'Augmented prompt:\\n {augmented_prompt}')\n",
    "\n",
    "  # TODO: \"G\" like \"Generation\": Generate a response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "BeRJbjeZcn-i",
    "outputId": "bac7f66a-7551-4539-893e-6647d8c25529"
   },
   "outputs": [],
   "source": [
    "# Define user input. This should be a question regarding the ingested book\n",
    "user_input = \"Lucy noticed a number on the ceiling when taking breakfast. which number was written into the ceiling?\" # The answer should contain the number \"28\"\n",
    "\n",
    "# TODO: Perform rag\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
